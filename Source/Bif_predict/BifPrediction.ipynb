{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Bifurcation Prediction Model\n",
    " This notebook trains and tests several regression models to predict the optimum bifurcation point for a terminal node/segment pair. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import basic modules\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "#Import ML modules\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import callbacks\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import RFECV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing\n",
    "We will load the dataset from the 06_Data directory (stored in author's pc) and load the dataset. Determine which dataset to load by specifying in the 'load_path' line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/taigakobayashi/OneDrive - University of Bath/FYP/06_Data\n"
     ]
    }
   ],
   "source": [
    "#Change directory to 06_Data folder\n",
    "os.chdir('../../06_Data')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Px</th>\n",
       "      <th>Py</th>\n",
       "      <th>D1x</th>\n",
       "      <th>D1y</th>\n",
       "      <th>D2x</th>\n",
       "      <th>D2y</th>\n",
       "      <th>Radius</th>\n",
       "      <th>circleradius</th>\n",
       "      <th>totseg</th>\n",
       "      <th>...</th>\n",
       "      <th>GPx</th>\n",
       "      <th>GPy</th>\n",
       "      <th>OrigSeg</th>\n",
       "      <th>l1</th>\n",
       "      <th>l2</th>\n",
       "      <th>l3</th>\n",
       "      <th>beta</th>\n",
       "      <th>alpha</th>\n",
       "      <th>Bifx</th>\n",
       "      <th>Bify</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.003365</td>\n",
       "      <td>0.002243</td>\n",
       "      <td>0.003175</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000032</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.000470</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>168.61</td>\n",
       "      <td>39.196</td>\n",
       "      <td>0.003182</td>\n",
       "      <td>0.001652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.002201</td>\n",
       "      <td>-0.000643</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>-0.001301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.003291</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000329</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>1</td>\n",
       "      <td>0.001357</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>114.62</td>\n",
       "      <td>77.905</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>-0.000841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.003325</td>\n",
       "      <td>0.004009</td>\n",
       "      <td>-0.002983</td>\n",
       "      <td>0.002643</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>0.003415</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003415</td>\n",
       "      <td>-0.006830</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.003266</td>\n",
       "      <td>0.003222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.003965</td>\n",
       "      <td>-0.000743</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>-0.002590</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003535</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000205</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.004008</td>\n",
       "      <td>134.69</td>\n",
       "      <td>62.048</td>\n",
       "      <td>0.001881</td>\n",
       "      <td>-0.001184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>-0.002749</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>-0.002955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.003651</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.004905</td>\n",
       "      <td>138.86</td>\n",
       "      <td>19.348</td>\n",
       "      <td>-0.001194</td>\n",
       "      <td>-0.000346</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        Px        Py       D1x       D1y  D2x  D2y  Radius  \\\n",
       "0           0  0.003365  0.002243  0.003175  0.000979  0.0  0.0       5   \n",
       "1           1  0.002201 -0.000643  0.001378 -0.001301  0.0  0.0       3   \n",
       "2           2 -0.003325  0.004009 -0.002983  0.002643  0.0  0.0      13   \n",
       "3           3  0.003965 -0.000743  0.000408 -0.002590  0.0  0.0       1   \n",
       "4           4 -0.002749  0.000490  0.000743 -0.002955  0.0  0.0       1   \n",
       "\n",
       "   circleradius  totseg  ...       GPx       GPy  OrigSeg        l1        l2  \\\n",
       "0      0.003162      12  ... -0.000032  0.000632        1  0.000633  0.000470   \n",
       "1      0.003291      13  ... -0.000329  0.001316        1  0.001357  0.000736   \n",
       "2      0.003415      14  ... -0.003415 -0.006830        1  0.000000  0.000000   \n",
       "3      0.003535      15  ...  0.000205  0.000661        0  0.000692  0.000753   \n",
       "4      0.003651      16  ... -0.000063  0.000841        0  0.000844  0.000689   \n",
       "\n",
       "         l3    beta   alpha      Bifx      Bify  \n",
       "0  0.001279  168.61  39.196  0.003182  0.001652  \n",
       "1  0.001054  114.62  77.905  0.001729 -0.000841  \n",
       "2  0.000000     0.0   0.000 -0.003266  0.003222  \n",
       "3  0.004008  134.69  62.048  0.001881 -0.001184  \n",
       "4  0.004905  138.86  19.348 -0.001194 -0.000346  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read dataset path as pandas dataframe\n",
    "load_path = (os.getcwd() + \"/ProcessedData/WP2T1/3000_seg_enriched_dataset.csv\")#change to desired load path\n",
    "\n",
    "#Load file as pandas dataframe\n",
    "df = pd.read_csv(load_path,dtype={'beta': object}) #Specify 'beta' column as object as it contains imaginary numbers\n",
    "\n",
    "#See summary of dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some points contain NaN values due to errors in MATLAB output and concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN count: 0\n",
      "(299000, 22)\n",
      "NaN count: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(299000, 22)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display number of NaN rows\n",
    "print('NaN count:',df.isna().values.sum())\n",
    "\n",
    "#Remove NaN rows\n",
    "df = df.dropna()\n",
    "\n",
    "#It should display 'NaN count: 0' here\n",
    "print('NaN count:',df.isna().values.sum())\n",
    "\n",
    "#Check shape of dataframe\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to errors in MATLAB code generation, there are some values in Beta (e.g. '180-28i') that need to be removed. Column 'beta' can then be converted to a float column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126 rows removed out of 299000\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 298874 entries, 0 to 298999\n",
      "Data columns (total 22 columns):\n",
      " #   Column        Non-Null Count   Dtype  \n",
      "---  ------        --------------   -----  \n",
      " 0   Unnamed: 0    298874 non-null  int64  \n",
      " 1   Px            298874 non-null  float64\n",
      " 2   Py            298874 non-null  float64\n",
      " 3   D1x           298874 non-null  float64\n",
      " 4   D1y           298874 non-null  float64\n",
      " 5   D2x           298874 non-null  float64\n",
      " 6   D2y           298874 non-null  float64\n",
      " 7   Radius        298874 non-null  int64  \n",
      " 8   circleradius  298874 non-null  float64\n",
      " 9   totseg        298874 non-null  int64  \n",
      " 10  P2x           298874 non-null  float64\n",
      " 11  P2y           298874 non-null  float64\n",
      " 12  GPx           298874 non-null  float64\n",
      " 13  GPy           298874 non-null  float64\n",
      " 14  OrigSeg       298874 non-null  int64  \n",
      " 15  l1            298874 non-null  float64\n",
      " 16  l2            298874 non-null  float64\n",
      " 17  l3            298874 non-null  float64\n",
      " 18  beta          298874 non-null  float64\n",
      " 19  alpha         298874 non-null  float64\n",
      " 20  Bifx          298874 non-null  float64\n",
      " 21  Bify          298874 non-null  float64\n",
      "dtypes: float64(18), int64(4)\n",
      "memory usage: 52.4 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(298874, 22)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display number of rows to remove due to a row in 'beta' being imaginary\n",
    "print(df[df['beta'].str.contains(\"i\")].beta.count(),'rows removed out of',df.shape[0])\n",
    "\n",
    "#Remove rows containing 'i' \n",
    "df = df[~df['beta'].str.contains(\"i\")]\n",
    "\n",
    "#Convert column 'beta' into a column of type float\n",
    "df.beta = df.beta.astype(float)\n",
    "\n",
    "#Show summary of dataframe\n",
    "df.info()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional step - to output processed dataframe for use in MATLAB ML model\n",
    "df.to_csv(os.getcwd() + \"/ProcessedData/WP2T1/3000_seg_enriched_dataset_processed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional Normalisation step\n",
    "This normalises connection points Px, Py, D1x, D1y, Bifx, Bify, GPx, GPy, P2x and P2y in hopes of a more accurate representation of errors computed in the regression model. The normalised connection points are now all represented as a distance percentage relative to the range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-fa0e877bf0bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrangex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Px'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'D1x'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'D2x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Px'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'D1x'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'D2x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrangey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Py'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'D1y'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'D2y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Py'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'D1y'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'D2y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Bifx'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Px'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'D1x'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'GPx'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'P2x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Bifx'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Px'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'D1x'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'GPx'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'P2x'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrangex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Bify'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Py'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'D1y'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'GPy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'P2y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Bify'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Py'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'D1y'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'GPy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'P2y'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivide\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrangey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#Define the x and y range of the allocated area\n",
    "rangex = df[['Px','D1x','D2x']].max(axis=1) - df[['Px','D1x','D2x']].min(axis=1)\n",
    "rangey = df[['Py','D1y','D2y']].max(axis=1) - df[['Py','D1y','D2y']].min(axis=1)\n",
    "\n",
    "#Divide all connection points by range x and y\n",
    "df[['Bifx','Px','D1x','GPx','P2x']] = df[['Bifx','Px','D1x','GPx','P2x']].divide(rangex,axis=0)\n",
    "df[['Bify','Py','D1y','GPy','P2y']] = df[['Bify','Py','D1y','GPy','P2y']].divide(rangey,axis=0)\n",
    "\n",
    "#Drop any NaN rows resulting from this calculation\n",
    "print('NaN count:',df.isna().values.sum())\n",
    "df = df.dropna()\n",
    "\n",
    "#Summarise dataframe with normalised connections\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional step - to output processed dataframe for use in MATLAB ML model for normalised features\n",
    "df.to_csv(os.getcwd() + \"/ProcessedData/WP2T1/3000_seg_enriched_dataset_normalised.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting features and labels\n",
    "We will start with all features generated on the MATLAB code.\n",
    "Our labels are Bifx and Bify, corresponding to the x and y coordinates of the bifurcation point respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X size:  (8910, 17)\n",
      "y size:  (8910, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>D1x</th>\n",
       "      <th>D1y</th>\n",
       "      <th>Px</th>\n",
       "      <th>Py</th>\n",
       "      <th>Radius</th>\n",
       "      <th>circleradius</th>\n",
       "      <th>totseg</th>\n",
       "      <th>P2x</th>\n",
       "      <th>P2y</th>\n",
       "      <th>GPx</th>\n",
       "      <th>GPy</th>\n",
       "      <th>OrigSeg</th>\n",
       "      <th>l1</th>\n",
       "      <th>l2</th>\n",
       "      <th>l3</th>\n",
       "      <th>beta</th>\n",
       "      <th>alpha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.938275</td>\n",
       "      <td>0.415783</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.017235</td>\n",
       "      <td>12.0</td>\n",
       "      <td>-0.113183</td>\n",
       "      <td>-0.146033</td>\n",
       "      <td>-0.010327</td>\n",
       "      <td>0.292151</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.003451</td>\n",
       "      <td>0.002562</td>\n",
       "      <td>0.006971</td>\n",
       "      <td>168.61</td>\n",
       "      <td>39.196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.627191</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.438215</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.017938</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.149157</td>\n",
       "      <td>-0.561785</td>\n",
       "      <td>-0.149157</td>\n",
       "      <td>1.123884</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.007396</td>\n",
       "      <td>0.004011</td>\n",
       "      <td>0.005743</td>\n",
       "      <td>114.62</td>\n",
       "      <td>77.905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.201092</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.124810</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.018615</td>\n",
       "      <td>14.0</td>\n",
       "      <td>-0.012826</td>\n",
       "      <td>-0.373105</td>\n",
       "      <td>0.038895</td>\n",
       "      <td>0.332638</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003609</td>\n",
       "      <td>0.003946</td>\n",
       "      <td>0.019569</td>\n",
       "      <td>131.62</td>\n",
       "      <td>57.820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.893495</td>\n",
       "      <td>0.679834</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.019269</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-1.064998</td>\n",
       "      <td>-1.600748</td>\n",
       "      <td>-1.064998</td>\n",
       "      <td>-1.600748</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.233624</td>\n",
       "      <td>-0.881298</td>\n",
       "      <td>-0.766376</td>\n",
       "      <td>0.118702</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.019901</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.079345</td>\n",
       "      <td>-0.150851</td>\n",
       "      <td>-0.030382</td>\n",
       "      <td>0.249008</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.004988</td>\n",
       "      <td>0.003330</td>\n",
       "      <td>0.026930</td>\n",
       "      <td>144.01</td>\n",
       "      <td>16.734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        D1x       D1y        Px        Py  Radius  circleradius  totseg  \\\n",
       "0  0.938275  0.415783  1.000000  1.000000     5.0      0.017235    12.0   \n",
       "1  0.627191 -1.000000  1.000000 -0.438215     3.0      0.017938    13.0   \n",
       "2  0.201092 -1.000000  1.000000 -0.124810     1.0      0.018615    14.0   \n",
       "3 -0.893495  0.679834 -1.000000  1.000000    14.0      0.019269    15.0   \n",
       "4  0.233624 -0.881298 -0.766376  0.118702     1.0      0.019901    16.0   \n",
       "\n",
       "        P2x       P2y       GPx       GPy  OrigSeg        l1        l2  \\\n",
       "0 -0.113183 -0.146033 -0.010327  0.292151      1.0  0.003451  0.002562   \n",
       "1  0.149157 -0.561785 -0.149157  1.123884      1.0  0.007396  0.004011   \n",
       "2 -0.012826 -0.373105  0.038895  0.332638      0.0  0.003609  0.003946   \n",
       "3 -1.064998 -1.600748 -1.064998 -1.600748      1.0  0.000000  0.000000   \n",
       "4  0.079345 -0.150851 -0.030382  0.249008      0.0  0.004988  0.003330   \n",
       "\n",
       "         l3    beta   alpha  \n",
       "0  0.006971  168.61  39.196  \n",
       "1  0.005743  114.62  77.905  \n",
       "2  0.019569  131.62  57.820  \n",
       "3  0.000000    0.00   0.000  \n",
       "4  0.026930  144.01  16.734  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Identify bifurcation points as target y\n",
    "labels = ['Bifx','Bify']\n",
    "y = df[labels] \n",
    "\n",
    "#identify features x\n",
    "features = ['D1x', 'D1y', 'Px', 'Py','Radius','circleradius','totseg','P2x','P2y','GPx','GPy','OrigSeg','l1','l2','l3','beta','alpha']\n",
    "X = df[features]\n",
    "\n",
    "#print the shape of features and labels to see if they are appropriate\n",
    "print('X size: ', X.shape)\n",
    "print('y size: ', y.shape)\n",
    "\n",
    "#Show summary of feature dataframe\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression model setup\n",
    "We will be using 5 models:\n",
    "* Linear regressor\n",
    "* Decision tree regressor\n",
    "* Random forest regressor\n",
    "* XGBoost regressor\n",
    "* Neural Network\n",
    "\n",
    "### Standard models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modules\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "#Define regression models\n",
    "LinearRegressor_model = LinearRegression()\n",
    "TreeRegressor_model = DecisionTreeRegressor()\n",
    "RFRegressor_model = RandomForestRegressor(n_estimators = 128)\n",
    "XGBRegressor_model = MultiOutputRegressor(XGBRegressor(n_estimators = 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network\n",
    "The neural network will have one hidden layer, with a ReLu activation function that has been shown to be the best one for regression predictions. The output layer does not have an activation function as it must output a continuous value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'KerasRegressor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5272b9b03b79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mNNRegressor_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKerasRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbaseline_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'KerasRegressor' is not defined"
     ]
    }
   ],
   "source": [
    "# define base model\n",
    "def baseline_model():\n",
    "    \n",
    "    # create model using keras layers\n",
    "    model = Sequential() #Define keras model\n",
    "    model.add(Dense(X.shape[1], input_dim=X.shape[1], activation='relu')) #input layer\n",
    "    model.add(Dense(X.shape[1],activation='relu')) #hidden layer\n",
    "    model.add(Dense(2)) #output layer (2 neurons for x and y bif point)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# evaluate model\n",
    "NNRegressor_model = KerasRegressor(build_fn=baseline_model, epochs=50, batch_size=50, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training & Testing models\n",
    "Each model will be trained and its performance evaluated with 10 fold cross validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an array of regression models to loop through\n",
    "regression_models = [LinearRegressor_model,\n",
    "                     TreeRegressor_model,\n",
    "                     RFRegressor_model,\n",
    "                     XGBRegressor_model,\n",
    "                     NNRegressor_model\n",
    "                    ]\n",
    "\n",
    "#Similar to the above but with the names in the form of strings\n",
    "regression_model_name = ['LinearRegressor_model',\n",
    "                         'TreeRegressor_model',\n",
    "                         'RFRegressor_model',\n",
    "                         'XGBRegressor_model',\n",
    "                         'NNRegressor_model'\n",
    "                        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split dataframe into train and test dataset for use in final prediction evaluation\n",
    "traindf=df.sample(frac=0.8,random_state=200) #random state is a seed value\n",
    "testdf=df.drop(traindf.index).reset_index(drop=True)\n",
    "\n",
    "#Redefine features and labels for train and test dataset\n",
    "y_train = traindf[labels]\n",
    "X_train = traindf[features]\n",
    "y_test = testdf[labels]\n",
    "X_test = testdf[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegressor_model training time: 0.3711280822753906 prediction time: 0.004683971405029297\n",
      "             cross validation error:  0.06597330450753117\n",
      "             cross validation std dev:  0.0013172421692533784\n",
      "TreeRegressor_model training time: 2.234699010848999 prediction time: 0.005164623260498047\n",
      "             cross validation error:  0.06103013021189295\n",
      "             cross validation std dev:  0.001593239626999105\n",
      "RFRegressor_model training time: 154.51014614105225 prediction time: 0.13533711433410645\n",
      "             cross validation error:  0.042385408395724085\n",
      "             cross validation std dev:  0.0013051318355229938\n",
      "XGBRegressor_model training time: 88.40206098556519 prediction time: 0.016638994216918945\n",
      "             cross validation error:  0.04217334681462712\n",
      "             cross validation std dev:  0.0011603555416015853\n",
      "Epoch 1/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 2.6377\n",
      "Epoch 2/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3296\n",
      "Epoch 3/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1979\n",
      "Epoch 4/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1070\n",
      "Epoch 5/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1073\n",
      "Epoch 6/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0961\n",
      "Epoch 7/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0922\n",
      "Epoch 8/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0968\n",
      "Epoch 9/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0954\n",
      "Epoch 10/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0880\n",
      "Epoch 11/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0954\n",
      "Epoch 12/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0970\n",
      "Epoch 13/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0858\n",
      "Epoch 14/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0923\n",
      "Epoch 15/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0926\n",
      "Epoch 16/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0829\n",
      "Epoch 17/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0953\n",
      "Epoch 18/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0872\n",
      "Epoch 19/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0848\n",
      "Epoch 20/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0928\n",
      "Epoch 21/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0938\n",
      "Epoch 22/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0880\n",
      "Epoch 23/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0907\n",
      "Epoch 24/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0857\n",
      "Epoch 25/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0886\n",
      "Epoch 26/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0989\n",
      "Epoch 27/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0923\n",
      "Epoch 28/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0876\n",
      "Epoch 29/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0891\n",
      "Epoch 30/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0866\n",
      "Epoch 31/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0864\n",
      "Epoch 32/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0841\n",
      "Epoch 33/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0861\n",
      "Epoch 34/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0902\n",
      "Epoch 35/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0866\n",
      "Epoch 36/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0862\n",
      "Epoch 37/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0883\n",
      "Epoch 38/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0888\n",
      "Epoch 39/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0900\n",
      "Epoch 40/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0855\n",
      "Epoch 41/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0936\n",
      "Epoch 42/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0842\n",
      "Epoch 43/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0946\n",
      "Epoch 44/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0881\n",
      "Epoch 45/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0894\n",
      "Epoch 46/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0933\n",
      "Epoch 47/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0876\n",
      "Epoch 48/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0844\n",
      "Epoch 49/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0919\n",
      "Epoch 50/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0887\n",
      "18/18 [==============================] - 0s 959us/step\n",
      "Epoch 1/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 6.1679\n",
      "Epoch 2/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.7532\n",
      "Epoch 3/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.4153\n",
      "Epoch 4/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2805\n",
      "Epoch 5/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2108\n",
      "Epoch 6/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1511\n",
      "Epoch 7/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1130\n",
      "Epoch 8/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0926\n",
      "Epoch 9/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1046\n",
      "Epoch 10/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0958\n",
      "Epoch 11/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0895\n",
      "Epoch 12/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0880\n",
      "Epoch 13/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0861\n",
      "Epoch 14/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0847\n",
      "Epoch 15/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0808\n",
      "Epoch 16/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0849\n",
      "Epoch 17/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0828\n",
      "Epoch 18/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0885\n",
      "Epoch 19/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0817\n",
      "Epoch 20/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0787\n",
      "Epoch 21/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0807\n",
      "Epoch 22/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0780\n",
      "Epoch 23/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0796\n",
      "Epoch 24/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0870\n",
      "Epoch 25/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0800\n",
      "Epoch 26/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0818\n",
      "Epoch 27/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0829\n",
      "Epoch 28/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0790\n",
      "Epoch 29/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0815\n",
      "Epoch 30/50\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.0810\n",
      "Epoch 31/50\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.0829A: 0s - loss: 0.08\n",
      "Epoch 32/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0786\n",
      "Epoch 33/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0812\n",
      "Epoch 34/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0807\n",
      "Epoch 35/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0780\n",
      "Epoch 36/50\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.0771\n",
      "Epoch 37/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0776\n",
      "Epoch 38/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0772\n",
      "Epoch 39/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0786\n",
      "Epoch 41/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0792\n",
      "Epoch 42/50\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.0784\n",
      "Epoch 43/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0780\n",
      "Epoch 44/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0808\n",
      "Epoch 45/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0805\n",
      "Epoch 46/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0784\n",
      "Epoch 47/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0761\n",
      "Epoch 48/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0805\n",
      "Epoch 49/50\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.0765\n",
      "Epoch 50/50\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.0810\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "Epoch 1/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.9461\n",
      "Epoch 2/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2861\n",
      "Epoch 3/50\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.1834A: 1s - loss\n",
      "Epoch 4/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1478\n",
      "Epoch 5/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1258\n",
      "Epoch 6/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1119\n",
      "Epoch 7/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0974\n",
      "Epoch 8/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1083\n",
      "Epoch 9/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0920\n",
      "Epoch 10/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0960\n",
      "Epoch 11/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0897\n",
      "Epoch 12/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0905\n",
      "Epoch 13/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0933\n",
      "Epoch 14/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0887\n",
      "Epoch 15/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0909\n",
      "Epoch 16/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0857\n",
      "Epoch 17/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0907\n",
      "Epoch 18/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0852\n",
      "Epoch 19/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0862\n",
      "Epoch 20/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0844\n",
      "Epoch 21/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0882\n",
      "Epoch 22/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0892\n",
      "Epoch 23/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0902\n",
      "Epoch 24/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0819\n",
      "Epoch 25/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0886\n",
      "Epoch 26/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0855\n",
      "Epoch 27/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0796\n",
      "Epoch 28/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0838\n",
      "Epoch 29/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0855\n",
      "Epoch 30/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0817\n",
      "Epoch 31/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0817\n",
      "Epoch 32/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0835\n",
      "Epoch 33/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0880\n",
      "Epoch 34/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0825\n",
      "Epoch 35/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0895\n",
      "Epoch 36/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0797\n",
      "Epoch 37/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0822\n",
      "Epoch 38/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0821\n",
      "Epoch 39/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0866\n",
      "Epoch 40/50\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.0832\n",
      "Epoch 41/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0804\n",
      "Epoch 42/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0838\n",
      "Epoch 43/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0821\n",
      "Epoch 44/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0895\n",
      "Epoch 45/50\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.0814\n",
      "Epoch 46/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0859\n",
      "Epoch 47/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0805\n",
      "Epoch 48/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0812\n",
      "Epoch 49/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0809\n",
      "Epoch 50/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0809\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "Epoch 1/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 4.1492\n",
      "Epoch 2/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3202\n",
      "Epoch 3/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1462\n",
      "Epoch 4/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1056\n",
      "Epoch 5/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0973\n",
      "Epoch 6/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0988\n",
      "Epoch 7/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0934\n",
      "Epoch 8/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0891\n",
      "Epoch 9/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0872\n",
      "Epoch 10/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0895\n",
      "Epoch 11/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0871\n",
      "Epoch 12/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0887\n",
      "Epoch 13/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0880\n",
      "Epoch 14/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0929\n",
      "Epoch 15/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0889A: 0s - loss: 0.089\n",
      "Epoch 16/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0841\n",
      "Epoch 17/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0908\n",
      "Epoch 18/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0795\n",
      "Epoch 19/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0902\n",
      "Epoch 20/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0865\n",
      "Epoch 21/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0834\n",
      "Epoch 22/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0882\n",
      "Epoch 23/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0956\n",
      "Epoch 24/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0883\n",
      "Epoch 25/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0839\n",
      "Epoch 26/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0859\n",
      "Epoch 27/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0891\n",
      "Epoch 28/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0821\n",
      "Epoch 29/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0811\n",
      "Epoch 30/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0837\n",
      "Epoch 31/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0814\n",
      "Epoch 32/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0853\n",
      "Epoch 33/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0885\n",
      "Epoch 34/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0918\n",
      "Epoch 35/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0840\n",
      "Epoch 36/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0895\n",
      "Epoch 37/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0808\n",
      "Epoch 38/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0834\n",
      "Epoch 39/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0833\n",
      "Epoch 40/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0855\n",
      "Epoch 41/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0796\n",
      "Epoch 42/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0800\n",
      "Epoch 43/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0836\n",
      "Epoch 44/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0895\n",
      "Epoch 45/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0817\n",
      "Epoch 46/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0813\n",
      "Epoch 47/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0917\n",
      "Epoch 48/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0826\n",
      "Epoch 49/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0805\n",
      "Epoch 50/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0817\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "Epoch 1/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 2.7948\n",
      "Epoch 2/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3717\n",
      "Epoch 3/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1519\n",
      "Epoch 4/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1062\n",
      "Epoch 5/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0950\n",
      "Epoch 6/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0882\n",
      "Epoch 7/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0831\n",
      "Epoch 8/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0860\n",
      "Epoch 9/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0793\n",
      "Epoch 10/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0813\n",
      "Epoch 11/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0806\n",
      "Epoch 12/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0756\n",
      "Epoch 13/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0767\n",
      "Epoch 14/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0776\n",
      "Epoch 15/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0771\n",
      "Epoch 16/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0804A: 0s - loss: 0.081\n",
      "Epoch 17/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0793\n",
      "Epoch 18/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0741\n",
      "Epoch 19/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0761\n",
      "Epoch 20/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0779A: 0s - loss: 0.078\n",
      "Epoch 21/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0752\n",
      "Epoch 22/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0779\n",
      "Epoch 23/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0797A: 0s - loss: 0.0\n",
      "Epoch 24/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0788\n",
      "Epoch 25/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0754\n",
      "Epoch 26/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0760\n",
      "Epoch 27/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0801\n",
      "Epoch 28/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0751\n",
      "Epoch 29/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0785\n",
      "Epoch 30/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0832\n",
      "Epoch 31/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0800\n",
      "Epoch 32/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0750\n",
      "Epoch 33/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0750\n",
      "Epoch 34/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0744\n",
      "Epoch 35/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0757\n",
      "Epoch 36/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0755\n",
      "Epoch 37/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0777\n",
      "Epoch 38/50\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.0769\n",
      "Epoch 39/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0755\n",
      "Epoch 40/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0740\n",
      "Epoch 41/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0764\n",
      "Epoch 42/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0763\n",
      "Epoch 43/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0728\n",
      "Epoch 44/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0771\n",
      "Epoch 45/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0732\n",
      "Epoch 46/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0775\n",
      "Epoch 47/50\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.0818\n",
      "Epoch 48/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0777\n",
      "Epoch 49/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0768\n",
      "Epoch 50/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0737\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "Epoch 1/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 2.6675\n",
      "Epoch 2/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3694\n",
      "Epoch 3/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2104\n",
      "Epoch 4/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1264\n",
      "Epoch 5/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1020\n",
      "Epoch 6/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0923\n",
      "Epoch 7/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0900\n",
      "Epoch 8/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0896\n",
      "Epoch 9/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0869\n",
      "Epoch 10/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0801\n",
      "Epoch 11/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0815\n",
      "Epoch 12/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0801\n",
      "Epoch 13/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0809\n",
      "Epoch 14/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0804\n",
      "Epoch 15/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0822\n",
      "Epoch 16/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0858\n",
      "Epoch 17/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0804\n",
      "Epoch 18/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0820\n",
      "Epoch 19/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0809\n",
      "Epoch 20/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0794\n",
      "Epoch 21/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0806\n",
      "Epoch 22/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0785\n",
      "Epoch 23/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0841\n",
      "Epoch 24/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0788\n",
      "Epoch 25/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0765\n",
      "Epoch 26/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0776\n",
      "Epoch 27/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0772\n",
      "Epoch 28/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0786\n",
      "Epoch 29/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0762\n",
      "Epoch 30/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0803\n",
      "Epoch 31/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0821\n",
      "Epoch 32/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0812\n",
      "Epoch 33/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0792\n",
      "Epoch 34/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0802\n",
      "Epoch 35/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0792\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0795\n",
      "Epoch 37/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0786\n",
      "Epoch 38/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0780\n",
      "Epoch 39/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0774\n",
      "Epoch 40/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0798\n",
      "Epoch 41/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0748\n",
      "Epoch 42/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0798\n",
      "Epoch 43/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0825\n",
      "Epoch 44/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0805\n",
      "Epoch 45/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0741\n",
      "Epoch 46/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0772\n",
      "Epoch 47/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0829\n",
      "Epoch 48/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0768A: 0s - loss: 0\n",
      "Epoch 49/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0756\n",
      "Epoch 50/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0794\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "Epoch 1/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 2.9094\n",
      "Epoch 2/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3438\n",
      "Epoch 3/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2439\n",
      "Epoch 4/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1804\n",
      "Epoch 5/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1358\n",
      "Epoch 6/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1154\n",
      "Epoch 7/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1035\n",
      "Epoch 8/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0996\n",
      "Epoch 9/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0963\n",
      "Epoch 10/50\n",
      "161/161 [==============================] - ETA: 0s - loss: 0.0913- ETA: 0s - loss - 0s 3ms/step - loss: 0.0933\n",
      "Epoch 11/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0961\n",
      "Epoch 12/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0906\n",
      "Epoch 13/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0885\n",
      "Epoch 14/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0867\n",
      "Epoch 15/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0877\n",
      "Epoch 16/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0923\n",
      "Epoch 17/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0874\n",
      "Epoch 18/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0832\n",
      "Epoch 19/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0860\n",
      "Epoch 20/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0904\n",
      "Epoch 21/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0866\n",
      "Epoch 22/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0839\n",
      "Epoch 23/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0823\n",
      "Epoch 24/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0894\n",
      "Epoch 25/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0838\n",
      "Epoch 26/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0800\n",
      "Epoch 27/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0847\n",
      "Epoch 28/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0848\n",
      "Epoch 29/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0850\n",
      "Epoch 30/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0836\n",
      "Epoch 31/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0820\n",
      "Epoch 32/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0852\n",
      "Epoch 33/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0847\n",
      "Epoch 34/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0790\n",
      "Epoch 35/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0785\n",
      "Epoch 36/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0847\n",
      "Epoch 37/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0931\n",
      "Epoch 38/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0837\n",
      "Epoch 39/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0849\n",
      "Epoch 40/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0796A: 0s - loss: 0.079\n",
      "Epoch 41/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0895\n",
      "Epoch 42/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0851\n",
      "Epoch 43/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0836\n",
      "Epoch 44/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0819\n",
      "Epoch 45/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0797\n",
      "Epoch 46/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0814\n",
      "Epoch 47/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0854\n",
      "Epoch 48/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0846A: 0s - loss: 0. - ETA: 0s - loss: 0.084\n",
      "Epoch 49/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0817\n",
      "Epoch 50/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0775\n",
      "18/18 [==============================] - 0s 2ms/step\n",
      "Epoch 1/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.6195\n",
      "Epoch 2/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3124\n",
      "Epoch 3/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2257\n",
      "Epoch 4/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1519\n",
      "Epoch 5/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1258\n",
      "Epoch 6/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1155\n",
      "Epoch 7/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1373\n",
      "Epoch 8/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1159\n",
      "Epoch 9/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1051\n",
      "Epoch 10/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1081\n",
      "Epoch 11/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1003\n",
      "Epoch 12/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1010\n",
      "Epoch 13/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1083\n",
      "Epoch 14/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1035\n",
      "Epoch 15/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0990\n",
      "Epoch 16/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0991\n",
      "Epoch 17/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1152\n",
      "Epoch 18/50\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.0946\n",
      "Epoch 19/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1156\n",
      "Epoch 20/50\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.0988\n",
      "Epoch 21/50\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1066\n",
      "Epoch 22/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0949\n",
      "Epoch 23/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0859\n",
      "Epoch 24/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0963\n",
      "Epoch 25/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0901\n",
      "Epoch 26/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0984\n",
      "Epoch 27/50\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.1117\n",
      "Epoch 28/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1010\n",
      "Epoch 29/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1005\n",
      "Epoch 30/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0949\n",
      "Epoch 31/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0979\n",
      "Epoch 32/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0924\n",
      "Epoch 33/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0947\n",
      "Epoch 34/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0853\n",
      "Epoch 35/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1010\n",
      "Epoch 36/50\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.0915\n",
      "Epoch 37/50\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.0968\n",
      "Epoch 38/50\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.0943\n",
      "Epoch 39/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0991\n",
      "Epoch 40/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0961\n",
      "Epoch 41/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0880\n",
      "Epoch 42/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0933\n",
      "Epoch 43/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0933\n",
      "Epoch 44/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0913\n",
      "Epoch 45/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0893\n",
      "Epoch 46/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0880\n",
      "Epoch 47/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0968\n",
      "Epoch 48/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0932\n",
      "Epoch 49/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0925\n",
      "Epoch 50/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0948\n",
      "18/18 [==============================] - 0s 1ms/step\n",
      "Epoch 1/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 1.6856\n",
      "Epoch 2/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2895\n",
      "Epoch 3/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1770\n",
      "Epoch 4/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1187\n",
      "Epoch 5/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0941\n",
      "Epoch 6/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0868\n",
      "Epoch 7/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0914\n",
      "Epoch 8/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0924\n",
      "Epoch 9/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0854\n",
      "Epoch 10/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0893\n",
      "Epoch 11/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0836\n",
      "Epoch 12/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0833\n",
      "Epoch 13/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0822\n",
      "Epoch 14/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0898\n",
      "Epoch 15/50\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.0832\n",
      "Epoch 16/50\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.0826\n",
      "Epoch 17/50\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.0865\n",
      "Epoch 18/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0824\n",
      "Epoch 19/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0839\n",
      "Epoch 20/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0870\n",
      "Epoch 21/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1025\n",
      "Epoch 22/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0864\n",
      "Epoch 23/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0821\n",
      "Epoch 24/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0864\n",
      "Epoch 25/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0784\n",
      "Epoch 26/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0803\n",
      "Epoch 27/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0876\n",
      "Epoch 28/50\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.0842\n",
      "Epoch 29/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0826A: 0s - loss: 0.082\n",
      "Epoch 30/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0840\n",
      "Epoch 31/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0774\n",
      "Epoch 32/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0827\n",
      "Epoch 33/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0833\n",
      "Epoch 34/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0930\n",
      "Epoch 35/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0828\n",
      "Epoch 36/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0795\n",
      "Epoch 37/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0807\n",
      "Epoch 38/50\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.0851\n",
      "Epoch 39/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0803\n",
      "Epoch 40/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0767\n",
      "Epoch 41/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0838\n",
      "Epoch 42/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0789\n",
      "Epoch 43/50\n",
      "161/161 [==============================] - 0s 1ms/step - loss: 0.0803\n",
      "Epoch 44/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0824A: 0s - loss: 0.082\n",
      "Epoch 45/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0853\n",
      "Epoch 46/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0816\n",
      "Epoch 47/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0889\n",
      "Epoch 48/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0802\n",
      "Epoch 49/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0844\n",
      "Epoch 50/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0791\n",
      "18/18 [==============================] - 0s 3ms/step\n",
      "Epoch 1/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 2.5215\n",
      "Epoch 2/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.3823\n",
      "Epoch 3/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.2490\n",
      "Epoch 4/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1721\n",
      "Epoch 5/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1328\n",
      "Epoch 6/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1325\n",
      "Epoch 7/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1249\n",
      "Epoch 8/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0961\n",
      "Epoch 9/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0990A: 0s - loss: 0\n",
      "Epoch 10/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1040\n",
      "Epoch 11/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1028\n",
      "Epoch 12/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1009\n",
      "Epoch 13/50\n",
      "161/161 [==============================] - 0s 3ms/step - loss: 0.0827\n",
      "Epoch 14/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1058\n",
      "Epoch 15/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0914\n",
      "Epoch 16/50\n",
      "161/161 [==============================] - 1s 3ms/step - loss: 0.1155\n",
      "Epoch 17/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1053\n",
      "Epoch 18/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1134\n",
      "Epoch 19/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0993\n",
      "Epoch 20/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0910\n",
      "Epoch 21/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0968\n",
      "Epoch 22/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0949\n",
      "Epoch 23/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0852\n",
      "Epoch 24/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0801\n",
      "Epoch 25/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0790\n",
      "Epoch 26/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0942\n",
      "Epoch 27/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0966\n",
      "Epoch 28/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1013\n",
      "Epoch 29/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0974\n",
      "Epoch 30/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0857\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0824\n",
      "Epoch 32/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0886\n",
      "Epoch 33/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0954\n",
      "Epoch 34/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0999\n",
      "Epoch 35/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1071\n",
      "Epoch 36/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0818\n",
      "Epoch 37/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0849\n",
      "Epoch 38/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1045\n",
      "Epoch 39/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0875\n",
      "Epoch 40/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0903\n",
      "Epoch 41/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0822\n",
      "Epoch 42/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0940\n",
      "Epoch 43/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0856\n",
      "Epoch 44/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0781\n",
      "Epoch 45/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0915\n",
      "Epoch 46/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1004\n",
      "Epoch 47/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0934\n",
      "Epoch 48/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.1016\n",
      "Epoch 49/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0965\n",
      "Epoch 50/50\n",
      "161/161 [==============================] - 0s 2ms/step - loss: 0.0919\n",
      "18/18 [==============================] - ETA:  - 0s 1ms/step\n",
      "Epoch 1/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 7.8647\n",
      "Epoch 2/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.4780\n",
      "Epoch 3/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.2420\n",
      "Epoch 4/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.1673\n",
      "Epoch 5/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.1315\n",
      "Epoch 6/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.1201\n",
      "Epoch 7/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.1114\n",
      "Epoch 8/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.1141\n",
      "Epoch 9/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.1064\n",
      "Epoch 10/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.1012\n",
      "Epoch 11/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0922\n",
      "Epoch 12/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.1012\n",
      "Epoch 13/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0930\n",
      "Epoch 14/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0961\n",
      "Epoch 15/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0967\n",
      "Epoch 16/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0914\n",
      "Epoch 17/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0958\n",
      "Epoch 18/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0981\n",
      "Epoch 19/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.1006\n",
      "Epoch 20/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0906\n",
      "Epoch 21/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0990\n",
      "Epoch 22/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0908\n",
      "Epoch 23/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0950\n",
      "Epoch 24/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0821\n",
      "Epoch 25/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0836\n",
      "Epoch 26/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.1022\n",
      "Epoch 27/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0874\n",
      "Epoch 28/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0912\n",
      "Epoch 29/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0852\n",
      "Epoch 30/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0898\n",
      "Epoch 31/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0879\n",
      "Epoch 32/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0867\n",
      "Epoch 33/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0875\n",
      "Epoch 34/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0815\n",
      "Epoch 35/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0910\n",
      "Epoch 36/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0882\n",
      "Epoch 37/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0906\n",
      "Epoch 38/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0935\n",
      "Epoch 39/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0920\n",
      "Epoch 40/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0845\n",
      "Epoch 41/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0877\n",
      "Epoch 42/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0783\n",
      "Epoch 43/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0935\n",
      "Epoch 44/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0841\n",
      "Epoch 45/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0850\n",
      "Epoch 46/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0886\n",
      "Epoch 47/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0855\n",
      "Epoch 48/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0821\n",
      "Epoch 49/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0921\n",
      "Epoch 50/50\n",
      "143/143 [==============================] - 0s 2ms/step - loss: 0.0915\n",
      "36/36 [==============================] - 0s 1ms/step\n",
      "NNRegressor_model training time: 173.4951720237732 prediction time: 0.20922112464904785\n",
      "             cross validation error:  0.08995967292941583\n",
      "             cross validation std dev:  0.014639012698312857\n"
     ]
    }
   ],
   "source": [
    "#train and validate each model using a 10-fold cross validation\n",
    "for model,model_name in zip(regression_models,regression_model_name):\n",
    "    \n",
    "    #Start timer to measure training time\n",
    "    starttime = time.time()\n",
    "\n",
    "    #Cross validate models (fit + calculate error for each fold) and compute average error score\n",
    "    score = -(cross_val_score(model,X,y,cv=10,scoring='neg_mean_absolute_error'))\n",
    "    \n",
    "    #Find prediction computing time by validating on test dataset\n",
    "    model.fit(X_train,y_train)\n",
    "    pred_starttime = time.time()\n",
    "    predictions = model.predict(X_test)\n",
    "    pred_endtime = time.time()\n",
    "    endtime = time.time()\n",
    "    \n",
    "    #Print results\n",
    "    print(model_name,'training time:',str(endtime-starttime),'prediction time:',str(pred_endtime-pred_starttime))\n",
    "    print('             cross validation error: ',score.mean())\n",
    "    print('             cross validation std dev: ',score.std())   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions for test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegressor_model  error:  0.06549734552345132 computation time:  0.004389286041259766\n",
      "TreeRegressor_model  error:  0.060769334931472234 computation time:  0.0023832321166992188\n",
      "RFRegressor_model  error:  0.04274404227914008 computation time:  0.1593630313873291\n",
      "XGBRegressor_model  error:  0.04188607175847148 computation time:  0.01821422576904297\n",
      "36/36 [==============================] - 0s 2ms/step\n",
      "NNRegressor_model  error:  0.08192701580662157 computation time:  0.19259214401245117\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Px</th>\n",
       "      <th>Py</th>\n",
       "      <th>D1x</th>\n",
       "      <th>D1y</th>\n",
       "      <th>D2x</th>\n",
       "      <th>D2y</th>\n",
       "      <th>Radius</th>\n",
       "      <th>circleradius</th>\n",
       "      <th>totseg</th>\n",
       "      <th>...</th>\n",
       "      <th>Bifx_pred_LinearRegressor_model</th>\n",
       "      <th>Bify_pred_LinearRegressor_model</th>\n",
       "      <th>Bifx_pred_TreeRegressor_model</th>\n",
       "      <th>Bify_pred_TreeRegressor_model</th>\n",
       "      <th>Bifx_pred_RFRegressor_model</th>\n",
       "      <th>Bify_pred_RFRegressor_model</th>\n",
       "      <th>Bifx_pred_XGBRegressor_model</th>\n",
       "      <th>Bify_pred_XGBRegressor_model</th>\n",
       "      <th>Bifx_pred_NNRegressor_model</th>\n",
       "      <th>Bify_pred_NNRegressor_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.124810</td>\n",
       "      <td>0.201092</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.018615</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.536140</td>\n",
       "      <td>-0.399812</td>\n",
       "      <td>0.418146</td>\n",
       "      <td>-0.294074</td>\n",
       "      <td>0.453084</td>\n",
       "      <td>-0.347250</td>\n",
       "      <td>0.453077</td>\n",
       "      <td>-0.388447</td>\n",
       "      <td>0.512765</td>\n",
       "      <td>-0.335198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.399409</td>\n",
       "      <td>0.246810</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.020513</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.543147</td>\n",
       "      <td>0.549947</td>\n",
       "      <td>0.500473</td>\n",
       "      <td>0.588955</td>\n",
       "      <td>0.541758</td>\n",
       "      <td>0.521290</td>\n",
       "      <td>0.517239</td>\n",
       "      <td>0.506483</td>\n",
       "      <td>0.519263</td>\n",
       "      <td>0.598188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.667890</td>\n",
       "      <td>0.644745</td>\n",
       "      <td>-0.332110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.027250</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.681992</td>\n",
       "      <td>0.154569</td>\n",
       "      <td>0.659739</td>\n",
       "      <td>0.006817</td>\n",
       "      <td>0.688469</td>\n",
       "      <td>0.051741</td>\n",
       "      <td>0.686930</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.694749</td>\n",
       "      <td>0.230783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.946706</td>\n",
       "      <td>0.548073</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.028144</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.627882</td>\n",
       "      <td>0.756787</td>\n",
       "      <td>0.639675</td>\n",
       "      <td>0.699152</td>\n",
       "      <td>0.609133</td>\n",
       "      <td>0.755498</td>\n",
       "      <td>0.609251</td>\n",
       "      <td>0.838493</td>\n",
       "      <td>0.622556</td>\n",
       "      <td>0.800780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.712351</td>\n",
       "      <td>0.506658</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.031857</td>\n",
       "      <td>41.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.701132</td>\n",
       "      <td>0.612838</td>\n",
       "      <td>0.715352</td>\n",
       "      <td>0.554193</td>\n",
       "      <td>0.721413</td>\n",
       "      <td>0.565976</td>\n",
       "      <td>0.736662</td>\n",
       "      <td>0.582480</td>\n",
       "      <td>0.675878</td>\n",
       "      <td>0.690208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   Px        Py       D1x       D1y  D2x  D2y  Radius  \\\n",
       "0           2  1.0 -0.124810  0.201092 -1.000000  0.0  0.0     1.0   \n",
       "1           5  1.0  0.399409  0.246810  1.000000  0.0  0.0     1.0   \n",
       "2          18  1.0  0.667890  0.644745 -0.332110  0.0  0.0     1.0   \n",
       "3          20  1.0  0.946706  0.548073  1.000000  0.0  0.0     1.0   \n",
       "4          29  1.0  1.000000  0.712351  0.506658  0.0  0.0     1.0   \n",
       "\n",
       "   circleradius  totseg  ...  Bifx_pred_LinearRegressor_model  \\\n",
       "0      0.018615    14.0  ...                         0.536140   \n",
       "1      0.020513    17.0  ...                         0.543147   \n",
       "2      0.027250    30.0  ...                         0.681992   \n",
       "3      0.028144    32.0  ...                         0.627882   \n",
       "4      0.031857    41.0  ...                         0.701132   \n",
       "\n",
       "   Bify_pred_LinearRegressor_model  Bifx_pred_TreeRegressor_model  \\\n",
       "0                        -0.399812                       0.418146   \n",
       "1                         0.549947                       0.500473   \n",
       "2                         0.154569                       0.659739   \n",
       "3                         0.756787                       0.639675   \n",
       "4                         0.612838                       0.715352   \n",
       "\n",
       "   Bify_pred_TreeRegressor_model  Bifx_pred_RFRegressor_model  \\\n",
       "0                      -0.294074                     0.453084   \n",
       "1                       0.588955                     0.541758   \n",
       "2                       0.006817                     0.688469   \n",
       "3                       0.699152                     0.609133   \n",
       "4                       0.554193                     0.721413   \n",
       "\n",
       "   Bify_pred_RFRegressor_model  Bifx_pred_XGBRegressor_model  \\\n",
       "0                    -0.347250                      0.453077   \n",
       "1                     0.521290                      0.517239   \n",
       "2                     0.051741                      0.686930   \n",
       "3                     0.755498                      0.609251   \n",
       "4                     0.565976                      0.736662   \n",
       "\n",
       "   Bify_pred_XGBRegressor_model  Bifx_pred_NNRegressor_model  \\\n",
       "0                     -0.388447                     0.512765   \n",
       "1                      0.506483                     0.519263   \n",
       "2                      0.000946                     0.694749   \n",
       "3                      0.838493                     0.622556   \n",
       "4                      0.582480                     0.675878   \n",
       "\n",
       "   Bify_pred_NNRegressor_model  \n",
       "0                    -0.335198  \n",
       "1                     0.598188  \n",
       "2                     0.230783  \n",
       "3                     0.800780  \n",
       "4                     0.690208  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define prediction dataframe\n",
    "pred_df = []\n",
    "\n",
    "#predict for all models\n",
    "for model,model_name in zip(regression_models,regression_model_name):\n",
    "    \n",
    "    #Make prediction with test dataset\n",
    "    starttime = time.time()\n",
    "    predictions = model.predict(X_test)\n",
    "    endtime = time.time()\n",
    "    \n",
    "    #Append predictions for each model into 'test' dataframe \n",
    "    pred_df = pd.DataFrame(data = predictions, columns=['Bifx_pred_' + str(model_name),'Bify_pred_' + str(model_name)])\n",
    "    testdf = pd.concat([testdf,pred_df],axis=1)\n",
    "\n",
    "    #evaluate error of prediction\n",
    "    print(model_name,' error: ',mean_absolute_error(y_test, predictions),'computation time: ',str(endtime-starttime))\n",
    "\n",
    "#display summary of test dataframe\n",
    "testdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate errors based on Euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Px</th>\n",
       "      <th>Py</th>\n",
       "      <th>D1x</th>\n",
       "      <th>D1y</th>\n",
       "      <th>D2x</th>\n",
       "      <th>D2y</th>\n",
       "      <th>Radius</th>\n",
       "      <th>circleradius</th>\n",
       "      <th>totseg</th>\n",
       "      <th>...</th>\n",
       "      <th>Bifx_pred_LinearRegressor_model</th>\n",
       "      <th>Bify_pred_LinearRegressor_model</th>\n",
       "      <th>Bifx_pred_TreeRegressor_model</th>\n",
       "      <th>Bify_pred_TreeRegressor_model</th>\n",
       "      <th>Bifx_pred_RFRegressor_model</th>\n",
       "      <th>Bify_pred_RFRegressor_model</th>\n",
       "      <th>Bifx_pred_XGBRegressor_model</th>\n",
       "      <th>Bify_pred_XGBRegressor_model</th>\n",
       "      <th>Bifx_pred_NNRegressor_model</th>\n",
       "      <th>Bify_pred_NNRegressor_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.124810</td>\n",
       "      <td>0.201092</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.018615</td>\n",
       "      <td>14.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.536140</td>\n",
       "      <td>-0.399812</td>\n",
       "      <td>0.418146</td>\n",
       "      <td>-0.294074</td>\n",
       "      <td>0.453084</td>\n",
       "      <td>-0.347250</td>\n",
       "      <td>0.453077</td>\n",
       "      <td>-0.388447</td>\n",
       "      <td>0.512765</td>\n",
       "      <td>-0.335198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.399409</td>\n",
       "      <td>0.246810</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.020513</td>\n",
       "      <td>17.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.543147</td>\n",
       "      <td>0.549947</td>\n",
       "      <td>0.500473</td>\n",
       "      <td>0.588955</td>\n",
       "      <td>0.541758</td>\n",
       "      <td>0.521290</td>\n",
       "      <td>0.517239</td>\n",
       "      <td>0.506483</td>\n",
       "      <td>0.519263</td>\n",
       "      <td>0.598188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.667890</td>\n",
       "      <td>0.644745</td>\n",
       "      <td>-0.332110</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.027250</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.681992</td>\n",
       "      <td>0.154569</td>\n",
       "      <td>0.659739</td>\n",
       "      <td>0.006817</td>\n",
       "      <td>0.688469</td>\n",
       "      <td>0.051741</td>\n",
       "      <td>0.686930</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.694749</td>\n",
       "      <td>0.230783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.946706</td>\n",
       "      <td>0.548073</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.028144</td>\n",
       "      <td>32.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.627882</td>\n",
       "      <td>0.756787</td>\n",
       "      <td>0.639675</td>\n",
       "      <td>0.699152</td>\n",
       "      <td>0.609133</td>\n",
       "      <td>0.755498</td>\n",
       "      <td>0.609251</td>\n",
       "      <td>0.838493</td>\n",
       "      <td>0.622556</td>\n",
       "      <td>0.800780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.712351</td>\n",
       "      <td>0.506658</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.031857</td>\n",
       "      <td>41.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.701132</td>\n",
       "      <td>0.612838</td>\n",
       "      <td>0.715352</td>\n",
       "      <td>0.554193</td>\n",
       "      <td>0.721413</td>\n",
       "      <td>0.565976</td>\n",
       "      <td>0.736662</td>\n",
       "      <td>0.582480</td>\n",
       "      <td>0.675878</td>\n",
       "      <td>0.690208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0   Px        Py       D1x       D1y  D2x  D2y  Radius  \\\n",
       "0           2  1.0 -0.124810  0.201092 -1.000000  0.0  0.0     1.0   \n",
       "1           5  1.0  0.399409  0.246810  1.000000  0.0  0.0     1.0   \n",
       "2          18  1.0  0.667890  0.644745 -0.332110  0.0  0.0     1.0   \n",
       "3          20  1.0  0.946706  0.548073  1.000000  0.0  0.0     1.0   \n",
       "4          29  1.0  1.000000  0.712351  0.506658  0.0  0.0     1.0   \n",
       "\n",
       "   circleradius  totseg  ...  Bifx_pred_LinearRegressor_model  \\\n",
       "0      0.018615    14.0  ...                         0.536140   \n",
       "1      0.020513    17.0  ...                         0.543147   \n",
       "2      0.027250    30.0  ...                         0.681992   \n",
       "3      0.028144    32.0  ...                         0.627882   \n",
       "4      0.031857    41.0  ...                         0.701132   \n",
       "\n",
       "   Bify_pred_LinearRegressor_model  Bifx_pred_TreeRegressor_model  \\\n",
       "0                        -0.399812                       0.418146   \n",
       "1                         0.549947                       0.500473   \n",
       "2                         0.154569                       0.659739   \n",
       "3                         0.756787                       0.639675   \n",
       "4                         0.612838                       0.715352   \n",
       "\n",
       "   Bify_pred_TreeRegressor_model  Bifx_pred_RFRegressor_model  \\\n",
       "0                      -0.294074                     0.453084   \n",
       "1                       0.588955                     0.541758   \n",
       "2                       0.006817                     0.688469   \n",
       "3                       0.699152                     0.609133   \n",
       "4                       0.554193                     0.721413   \n",
       "\n",
       "   Bify_pred_RFRegressor_model  Bifx_pred_XGBRegressor_model  \\\n",
       "0                    -0.347250                      0.453077   \n",
       "1                     0.521290                      0.517239   \n",
       "2                     0.051741                      0.686930   \n",
       "3                     0.755498                      0.609251   \n",
       "4                     0.565976                      0.736662   \n",
       "\n",
       "   Bify_pred_XGBRegressor_model  Bifx_pred_NNRegressor_model  \\\n",
       "0                     -0.388447                     0.512765   \n",
       "1                      0.506483                     0.519263   \n",
       "2                      0.000946                     0.694749   \n",
       "3                      0.838493                     0.622556   \n",
       "4                      0.582480                     0.675878   \n",
       "\n",
       "   Bify_pred_NNRegressor_model  \n",
       "0                    -0.335198  \n",
       "1                     0.598188  \n",
       "2                     0.230783  \n",
       "3                     0.800780  \n",
       "4                     0.690208  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#normalise features to evaluate normalised error (if it hasn't already been normalised)\n",
    "rangex = testdf[['Px','D1x','D2x']].max(axis=1) - testdf[['Px','D1x','D2x']].min(axis=1)\n",
    "rangey = testdf[['Py','D1y','D2y']].max(axis=1) - testdf[['Py','D1y','D2y']].min(axis=1)\n",
    "\n",
    "testdf.iloc[:,[1,3,20,22,24,26,28,30]] = testdf.iloc[:,[1,3,20,22,24,26,28,30]].divide(rangex,axis=0)\n",
    "testdf.iloc[:,[2,4,21,23,25,27,29,31]] = testdf.iloc[:,[2,4,21,23,25,27,29,31]].divide(rangey,axis=0)\n",
    "\n",
    "testdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Normalised error</th>\n",
       "      <th>model</th>\n",
       "      <th>LinearRegressor_model</th>\n",
       "      <th>TreeRegressor_model</th>\n",
       "      <th>RFRegressor_model</th>\n",
       "      <th>XGBRegressor_model</th>\n",
       "      <th>NNRegressor_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.085665</td>\n",
       "      <td>0.170053</td>\n",
       "      <td>0.109897</td>\n",
       "      <td>0.069595</td>\n",
       "      <td>0.127429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.128359</td>\n",
       "      <td>0.130927</td>\n",
       "      <td>0.112114</td>\n",
       "      <td>0.083632</td>\n",
       "      <td>0.148231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.064856</td>\n",
       "      <td>0.084585</td>\n",
       "      <td>0.042622</td>\n",
       "      <td>0.091091</td>\n",
       "      <td>0.142130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.098150</td>\n",
       "      <td>0.156970</td>\n",
       "      <td>0.097870</td>\n",
       "      <td>0.014908</td>\n",
       "      <td>0.053977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.075513</td>\n",
       "      <td>0.015266</td>\n",
       "      <td>0.027307</td>\n",
       "      <td>0.047681</td>\n",
       "      <td>0.156692</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Normalised error model  LinearRegressor_model  TreeRegressor_model  \\\n",
       "0              NaN   NaN               0.085665             0.170053   \n",
       "1              NaN   NaN               0.128359             0.130927   \n",
       "2              NaN   NaN               0.064856             0.084585   \n",
       "3              NaN   NaN               0.098150             0.156970   \n",
       "4              NaN   NaN               0.075513             0.015266   \n",
       "\n",
       "   RFRegressor_model  XGBRegressor_model  NNRegressor_model  \n",
       "0           0.109897            0.069595           0.127429  \n",
       "1           0.112114            0.083632           0.148231  \n",
       "2           0.042622            0.091091           0.142130  \n",
       "3           0.097870            0.014908           0.053977  \n",
       "4           0.027307            0.047681           0.156692  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create error dataframe for normalised error\n",
    "error_df = pd.DataFrame(columns =['Normalised error','model'])\n",
    "\n",
    "#Loop through each model to find normalised error for each prediction\n",
    "for model,model_name in zip(regression_models,regression_model_name):\n",
    "    \n",
    "    #Predicted x and y bifurcation coordinate\n",
    "    Pred_x = testdf['Bifx_pred_' + str(model_name)].values #Bifx\n",
    "    Pred_y = testdf['Bify_pred_' + str(model_name)].values #Bify\n",
    "    \n",
    "    #Actual x and y bifurcation coordinate\n",
    "    Actual_x = testdf['Bifx'].values #Bifx\n",
    "    Actual_y = testdf['Bify'].values #Bify\n",
    "    \n",
    "    #Calculate Euclidean distance\n",
    "    e = ((Pred_x - Actual_x)**2 + (Pred_y - Actual_y)**2)**0.5\n",
    "\n",
    "    #Use this to create histogram. Commment out otherwise\n",
    "#     newdf = pd.DataFrame(data = e, columns = ['Normalised error'])\n",
    "#     newdf['model'] = newdf.apply(lambda x: model_name, axis=1)\n",
    "#     error_df = error_df.append(newdf, ignore_index=True)\n",
    "    \n",
    "    #Append results into error dataframe\n",
    "    error_df[model_name] = e\n",
    "    \n",
    "error_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsYAAAHxCAYAAABu/3o9AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAABkpUlEQVR4nO3deXiU1dnH8e9MNvbVKItLX6kCsqiIC4JQbRUVUWupUrSAyuJK3RBEBBUQAZdX60trKYp1R4tbFZertm5oLa6ggjuLbAFCyDozz3PO+8eQkJlMkklIZuaZ/D69vEpmMs+chIf48+Y+9/FZay0iIiIiIk2cP9kLEBERERFJBQrGIiIiIiIoGIuIiIiIAArGIiIiIiKAgrGIiIiICKBgLCIiIiICKBiLiIiIiACQmewF1FV+fjHGaPRyU9exYyt27ChK9jIkyXQfSDndCwK6DyTM7/fRvn3Ler3Wc8HYGKtgLAC6DwTQfSB76V4Q0H0g+0atFCIiIiIiKBiLiIiIiAAebKUQERGR1OS6Dvn5eThOMCnvv22bH2NMUt5bEi8zM5v27XPJyGi4OKtgLCIiIg0iPz+PZs1a0LJlJ3w+X8LfPzPTj+MoGDcF1lqKi3eTn5/Hfvt1brDrqpVCREREGoTjBGnZsk1SQrE0LT6fj5Yt2zT4304oGIuIiEiDUSiWRGmMe03BWEREREQEBWMRERGRlHPjjdfwyisv1fg5H3+8kt///vwErahpUDAWEREREUFTKURERET22ccfr+TBB/+PAw7oxPr162jevBkXXjiWZ599ivXr1/GLX5zCpEnX88ILy3j22afw+zPo0KED1157IwcffAjbt+cxe/ZMtm/fTqdOncjPz6+49o8//sB9991FQUEBxhhGjLiAs846J4lfbfpSMBYRERFpAGvWfMn110/h8MN7cP31k3jssSX88Y8PUlxczLnnnk6PHr144om/8ec/P0z79u155ZWXmDbtBh59dCl33z2PXr36MH785WzcuIGLLx4FgOM4TJ8+hVtuuZ3u3XtQVFTEZZddzM9+dmiSv9r0pGAsIiIi0gA6d+7C4Yf3AKBr1660bNmKrKws2rVrR8uWLfn3v//JKaecSvv27QE488zh3HffXWzevImVKz/kqquuAeDAAw+iX7/+AGzYsJ5NmzYyd+7tFe8TCAT45pu1HHLIzxL69TUFCsYiIiIiDSArKyvi48zMyJjl81UdMWZtuCrs8/mw1lY8Xn6amzGGli1bsWTJExXP7dy5g5YtW/HFF6sa+kto8rT5TkRERCQBjj76GP75z9cr+odffvlF2rZty4EHHsTxxw/gxReXAbBlyxY+/nglAAcffAg5OTm89torAGzduoXRoy9g7dqvkvNFpDlVjEVEREQSoF+//vh8fv7wh8swxtKuXTvmzbsXv9/PdddNYe7c27jwwhHk5u7PYYcdDoSr0HPn3s19993FE0/8DcdxGDfuMvr2PaoiPEvD8dnKdXsP2LGjCGM8tWRpBLm5rcnLK0z2MiTJdB9IOd0LqWHLlnV06nRI0t4/M9OP45ikvb8kXqx7zu/30bFjq3pdT60UIiIiIiIoGIuIiIiIAArG0gS4hfrrVREREamdgrGkNRMKUfDu21jHSfZSREREJMUpGEtac3buwASDOJWO1hQRERGJRcFY0ppTUBD+/90FSV6JiIiIpDrNMZa05hbu3vP/6jMWEUmG59/5vlGue+5JhzbKdaVpU8VY0ppbVBz+/+LiJK9EREQS7eOPV3LVVRMiHluz5kvuvHNWo77v4sUPcvbZQxk7dhRjx47iwgtHMHr0BXz++aeN+r5eddVVE2o8rGTz5k2MGDE8IWtRxVjSmikpCf9/aWmSVyIiIqmgR48jmDr1iEZ/n3POOY9LL51Y8fHSpU/wxz/ey6JFjzT6e0v9KRhL2rKOgwkGADBlCsYiIhKuIj/00F944IG/cNVVEzjiiF589tmn7NqVzzXXTGbAgIHs3LmDBQvuYOvWrfj9fiZOvJJjjz2evLxtzJ07i6KiQrZvz+PMM4czbtxlvPLKSyxf/g8KCnYxcOBgMjMj45Uxhq1bt9KmTVuAaq9fVFTE7Nkz2LhxI126dCUvbyt33HEXn3zyUcT1f/vbkTFfv3LlhyxceD8+n4/WrVtz6613kJWVya233syOHTsAuOSS8QwaNIT169cxf/4cCgt306xZc6655gZ69uzFnDm3UlBQwE8/beDyyycxaNDgmN/Hq66aQPfuPfj8808JBoNcdtnVPPPMU/z44/dccMEoLrjgQsrKypg3bzbffvs1fr+fkSMv4owzziIYDDJv3izWrPmKTp26UFCwq+K6jz66hH/96w1c13D88Sdw+eWTGudGqIaCsaStylVi67qYQAB/Tk4SVyQiIqkmFHJ48MGHeffdt1m06E8MGDCQ++67i2HDzmbQoCFs376dK664lCVLnuCNN17j1FOHcsYZZ1FUVMR55w1jxIiRAOTlbeOxx54hMzOTxYsf5IUXlvHOO29RWLgbay0nnjiIm26aAVDt9R9+eBEHH3wId955D2vWfMnEiRdXrLPy9WfOvCnm6x95ZDGTJ99Ez569ePzxR/j66zXs2LGdTp26sGDBfXzzzVpef/1VBg0awqxZt3DRRWMZMuQUVq9exfTpU3jyyWUAtG3blvnz7631e2etZdGiv/HQQ3/hf/93AY888hS7duUzdmw4GD/00IO0bduWRx9dyq5duxg/fgyHHdadDz/8AIDHH3+WDRvWM2bM7wD44IMVrF37FYsW/Q2fz8esWTN4/fXl9O17VEP+ltdIwVjSlhtVJTZlZQrGIiIS4fjjBwBw6KHdKNyzYXvlyg9Zt24df/3rgwA4jsNPP21k1Kjf8/HHK3niiUf54YfvcJwQZXv+XXP44T0iKsXlrRQ7dmznD3+4nF69+rDffvvVeP2VK//DjBmzgXDLx6GHdqu4XuXrV/f6QYMGM23aZE46aQgnnTSEY489gQ0b1vPgg//H9u3bGDBgEGPHXkpJSQkbN25kyJBTAOjduw9t2rRh/fp1ABxxRO+4vncnnDAQgE6dOtOrVx+aNWtGp06dKSoKb3j/6KOVTJ16CwDt2rXjpJMG88knH/Hppx9x9tnnAXDQQQfTp0/fiq/ryy9Xc+mlvwcgECjjgAM6KRiLNARbVhbxcXlbhYiISLns7GwAfD4f1loAXNdw//1/qmh92L59O+3bt+ePf7yXTZt+4tRTT2fw4F+wcuWHFa/Jqabw0rHjfkyZMp3rrruaI488mi5dulZ7fb/fjzEm5nUqX7+61x92WHcGDhzMihXvsHDh/fziF18wZsylPPHEs3zwwfu8997bPPXUYzH7nK0F13Vr/FqiVf4PgYyMjBjXNFEfg+s6gA+wVV5rjMv55/+OkSMvAqCwsJCMjIyIVovGpqkUkrZMWWQQtmUKxiIiUrtjjunPsmXPAPDDD98zevQFBAJlrFz5H0aN+j2nnPIr1q9fR17etmqDbGV9+hzJiScOZOHC+2u8fv/+x/PGG68C8N133/L999/h8/niXt/48WMoKSnm/PNHcf75o/j66zX8/e9Ps3jxg5xyyq+4/vqp5OfnY4ylS5euvPXWmwCsXr2KnTt3RFSoG0K/fsfy8ssvALBr1y7eeeffHH10f/r3P47XX38VYwxbtmxm1arPKz7/tddeoaSkBMdxuOmm6/n3v//ZoGuqjSrGkrZMQBVjEZFkS/a84c8//5RTTz2p4uPc3P3p0KFjja+59tobmT9/DmPGjMRayy233E6LFi256KKxzJo1g5ycHPbfvxM9ehzBpk0/xbWOiROv4qKLfstnn31a7fXHjr2UO+64jTFjRtKly4F07LhfzOptda+fOPFK5sy5jYyMDFq0aMGUKdPp0KEDt956M6NHX0BGRgZXXjmJ1q1bM2PGLBYsuIPFix8kKyubOXPmk5WVVbdvbi0uvngcd989j9GjL8AYw+jRl9C9ew+6dfs5P/zwHRdeOIJOnTpXBPJBgwbz7bdfM2HCWIxxOf74EznjjLPYsmVzg66rJj5b/ncAHrFjRxHGeGrJ0ghyc1uTl1fzoR2FH60ksHFDxcctDu9Oi56NP6JHEiee+0CaBt0LqWHLlnV06nRI0t4/M9OP49RewU1Vr732Cp07d6Fv36PYsmULV189gaeffh6/X3/BX51Y95zf76Njx1b1up4qxpK2bFSF2ARUMRYRkdR1yCE/Y8GCuRjj4vP5mTx5WlJD8W23TeeHH6qeXDho0GDGjbssCStqfArGkraig7ANBpO0EhERkdr16HEEixc/muxlVJg5c3ayl5Bwqs1L2ooOwkbBWERERGqgYCxpKzoI25CCsYiIiFRPwVjSknUc7J55jOVUMRYREZGaKBhLWooVgm0olISViIiIiFdo852kpVhtE9Z1sa6LL8bpPCIi0jgCK59rlOvm9P91rZ9z993zWLXqMxwnxMaNG/jZz8IzlX/725EMG3Z2vd/7qqsmkJe3jebNWwBQXFxMly5dmTlzVq0zkpuiQYP68+67K6t9/pVXXuKTTz7i5ptvTdyiqqFgLGnJBmNXh20oiC+jeYJXIyIiyXD99VMA2Lx5E1dfPZElS55osGtPmTKdfv36A2CMYfr0KTz11ONcccWkBnsPSTwFY0lLppqNdiYUwt9MwVhEpCkbMWI4RxzRm2++WcvChX/lgw9W8MwzT2KMpXv3Hlx33RRycnL44IMVLF78ZxzHoXPnrkyZcjNt27arcr2yslIKCnZxxBG9APjqqy+4//57CATKaNu2HZMnT6NLl658//23zJlzG67rcuSRR/HBByt4+unnmTPnVgoKCvjppw1cfvkkOnbsGPP1Tz31GMuXv4zf76Nnz17ceOPNfPvtN8yfPwfXdcnOzmbatJkcdNDBvPfeOyxa9CesNXTp0pXJk6fRoUPHKl97+/YdYn6Pzj57KCedNIQvv1xNhw77MWzY2Tz77FPk5W1j2rSZHH30Maxfv4758+dQWLibZs2ac801N9CzZy82b97E7bffQmlpKb169a64ZklJCffcM4/vv/8OYwwXXjiaU089vVF+j+tLPcaSlqqvGKvPWERE4IQTTuTJJ5eRn5/PSy89z5/+9BBLljxB+/YdePLJR8nPz+fPf36Au+9+gIcffoLjjjuBP/3pjxWvnzdvNmPG/I5zzhnKhAkXc+yxx3PBBRcSCoW4887ZzJw5h4ceepyRIy9i3rw5AMyefSvjxoUr1126dMWttEm8bdu2PP74sxx//ICYr3ddl8ceW8LixY+yePFjOI5DXt42li59gpEjL2Lx4kc5++xf88UXq8jP38mCBXcwd+5dPPLIU/TpcyT33DO/ytdeXSgG2LlzByeccCIPP/wEwWCAt9/+FwsX/pVLLpnA0qVPAjBr1i389rcjeeSRp7j66uuYPn0KwWCQe++dz5lnDmfJkifo0+fIims+8shiunfvyUMPPcb//d9f+NvfHuKnnzY22O9pQ1DFWNJSdQFYwVhERACOOCJcyfzkk5Vs3LiBiRMvBsBxQhx+eA++/HI1W7duYdKk8Alvxri0adO24vXlrRSrVn3G9Ok3MnjwL8jKyuL7779l06aNTJ16XcXnFhcXs3t3AVu2bGbAgEEADBt2Ds8881SV9WzYsC7m6zMyMujduy/jxo3mpJOGMHLkheTm7s+AAQO55575/Oc/Kxg4cDADB57EBx+soGfPXnTu3AWAs88+j0cfXVLlvWpzwgkDAejUqTN9+x4FwAEHdKKwcDclJSVs3LiRIUNOAaB37z60adOG9evX8cknH3HrreH/GDjttDO4885ZAKxc+SGBQBkvv/wiAGVlZTFP1ksmBWNJS5VbKXb6ymhjs8nEr2AsIiIA5OTkAOC6hlNO+RXXXDMZCP91v+u6fPrpR/TteyTz5t0LQCAQoLS0tMp1+vQ5khEjRnLbbdNZvPgxXDfculDez+y6Lvn5O/H7M7DWxrWeWK8HmDv3br74YhUffLCC66+fxIwZszj55F/Ru3df3nvvHZYufYL333+XgQNPiri2tTaiOl3+XrXJysqq+HVG1MZ1a02Vz7eWPe/jw5jw1+rz+fD7w681xuWWW2bRvXsPIFyVbtOmLa+/vjyu9SSCWikkLVUOwGsz89nmLwk/7jjJWpKIiKSgo48+hrff/jf5+Tux1nL33XNZuvQJjjiiN198sYr169cBsGTJX/m///vfmNe44IILKS4u5oUXlnHIIT9j9+7dfPbZJwC8/PKL3HrrzbRq1YquXQ/k/fffA+CNN17F5/NVuVZ1r8/Pz+eii37LoYf+nHHjLuPYY4/nu+++YcaMm/jqqy8599zfMG7cZaxdu4YjjujNl1+uYvPmTQC8+OIy+vU7pkG/by1btqJLl6689dabAKxevYqdO3dw6KHd6N//OF577RUA3nrrTYLBAAD9+h3L888/C8D27dsZM+Z3bN26pUHXta9UMZa0ZJ1wMC4hRJnPId8foItppYqxiIhEOOyww7n44vFMmnQZ1lp+/vPDueiiseTk5DB16gxmzLgJY1xycw9gxozbY14jOzubCROu4P7772bo0DOZNetO7rvvLoLBIC1atGT69NsAmD79NubOvZ1FixbSrdthMSu32dnZMV/fvn17zj7714wfP5qcnGYcfPAhDBt2Dkce2Y9582azZMkiMjOzuOGGqXTo0JHJk29m2rQbCIUcOnXqxNSpMxr8ezdjxiwWLLiDxYsfJCsrmzlz5pOVlcV1193IrFkzePHF5+jRoyctWrQE4JJLxnP33fP4/e/PxxjDFVdMomvXAyv+IyAV+GxNdf0UtGNHUUV5Xpqu3NzW5OUVVvv87g9WENy6lS3+Yr7K3Ekzm8mAUGdaHN6dFj2PSOBKpTHVdh9I06F7ITVs2bKOTp0OSdr7Z2b6cZyqf8WfKh5+eBHDh/+a/fbbj7feepPXX1/OnDkLkr0sT4t1z/n9Pjp2bFWv66liLGmpvDJc7Av/f5nPwcFUO8ZNRESksR1wQCeuvfYKMjMzad26DVOn3pK0tQQCZUyceEnM58aNm8igQUMSvKLUoGAsaam8l7jEt7enuNTn0DKkHmMREUmOM88czplnDk/2MgDIyWnWoAeepItG3XxXVFTEWWedxcaN4Rl1K1asYPjw4Zx22mnce++9jfnW0sSVV4xLo4KxNt+JiIhIdRotGH/22Wf87ne/48cffwTCs+qmTZvGwoULeeWVV1i9ejVvvfVWY729NHHlAbisUjAuw63YlCciIiISrdGC8dKlS5k5cyb7778/AJ9//jmHHHIIBx10EJmZmQwfPpxXX321sd5emjBrLdZxCOLisnejZkAVYxEREalBo/UYz5kzJ+Ljbdu2kZubW/Hx/vvvz9atWxvr7aUJs46DtZaAz414POBzseoxFhERkWokbPOdMSZikLW1NuZg69rUd/yGpJ/c3NYxH3dKSgi0zKEElyzf3pN6rPXRIsdf7evEm/T7KeV0LyTftm1+MjMj/zL6pW9fa5T3Gv7zoTEfj35/SW9+f8P+ez1hwbhTp07k5eVVfJyXl1fRZlEXmmMsUPPMUqdwN8XFAXb5Swll7q0aF9oARWXFmnWaRjS7VsrpXkgNxpgqc4Qb69/ZseYVR88x3rx5E7/73Xn87GeHAuFjjIuLiznjjLM488zhEc+VmzfvHv7xjxd44YVldOjQEYBQKEhGRgY33HATffse1Shfj5ddddUELrlkAv369Y/5/ObNm7j66ok8++xLDf7expgqf/Y9Mcf4yCOP5IcffmDdunUceOCB/OMf/+A3v/lNot5empDydonoVoqgz2BcB2sMPr8qCiIiTcF+++VGjCXbvj2PkSN/zS9/eVqV5yo755zzuPTSiRUfL136BH/8470sWvRIo69ZkidhwTgnJ4c777yTq6++mkAgwJAhQzj99NMT9fbShFg3HIyDRFYTLJYQBus6+PzZyViaiIgk2fbt27HWUlBQEPdrjDFs3bqVNm3aArBz5w4WLLiDrVu34vf7mTjxSo499niKioqYPXsGGzdupEuXruTlbeWOO+7ik08+Yvnyf1BQsIuBAwfz29+OjPn6lSs/ZOHC+/H5fLRu3Zpbb72DrKxMbr31Znbs2AGEj1UeNGgI69evY/78ORQW7qZZs+Zcc80N9OzZizlzbqWgoICfftrA5ZdPYtCgwTG/pquumkD37j34/PNPCQaDXHbZ1TzzzFP8+OP3XHDBKC644ELKysqYN2823377NX6/n5EjL+KMM84iGAwyb94s1qz5ik6dulBQsKviuo8+uoR//esNXNdw/PEncPnlk+r/m5UEjR6M33zzzYpfDxgwgBdffLGx31KauPKKcSiqYhx+zISfz1IwFhFpCrZvz2Ps2FEEgwEKCnbRo0cv7rjjLvbff/+K58qddtrpjBo1GoAXXljGO++8RWHhbqy1nHjiIG66aQYA9913F8OGnc2gQUPYvn07V1xxKUuWPMHDDy/i4IMP4c4772HNmi+ZOPHiimvn5W3jsceeITMzk5kzb4r5+kceWczkyTfRs2cvHn/8Eb7+eg07dmynU6cuLFhwH998s5bXX3+VQYOGMGvWLVx00ViGDDmF1atXMX36FJ58chkAbdu2Zf782s+LsNayaNHfeOihv/C//7uARx55il278hk7NhyMH3roQdq2bcujjy5l165djB8/hsMO686HH34AwOOPP8uGDesZM+Z3AHzwwQrWrv2KRYv+hs/nY9asGbz++nJPtZ/o5DtJP27sVgqAIG5FRVlERNJfebuEMYYHHriXH3/8gWOPPZ6tW7fE1UqxY8d2/vCHy+nVqw/77bcfACtXfsi6dev4618fBMBxHH76aSMrV/6HGTNmA9CjxxEcemi3iusdfngPMjMza3z9oEGDmTZtMiedNISTThrCsceewIYN63nwwf9j+/ZtDBgwiLFjL6WkpISNGzcyZMgpAPTu3Yc2bdqwfv06AI44ondc35sTThgIQKdOnenVqw/NmjWjU6fOFBWFe3Y/+mhlxbHV7dq146STBvPJJx/x6acfcfbZ5wFw0EEH06dP34qv68svV3Pppb8HwsdOH3BAJwVjkWQy5RVjqm7MCJZXjEVEpEnx+/1cccUfuPjiUTz55KOccsqpcb2uY8f9mDJlOtdddzVHHnk0Xbp0xXUN99//p4rWiu3bt9O+fXv8fj/GVP13D4RbSstV9/rDDuvOwIGDWbHiHRYuvJ9f/OILxoy5lCeeeJYPPnif9957m6eeeixmn7O14LpulfeqSXlQB8jIyKjyvLVRLYkWXNcBfFDpnIDy1xrjcv75v2PkyIsAKCwsJCMjI6LVItVpB5KkH7e8laLqD6eQKsYiIk1WZmYmV155DUuWLK7o2Y1Hnz5HcuKJA1m48H4AjjmmP8uWPQPADz98z+jRFxAIlNG///G88Ub48LLvvvuW77//LuZo2upeP378GEpKijn//FGcf/4ovv56DX//+9MsXvwgp5zyK66/fir5+fkYY+nSpStvvRVuV129ehU7d+6IqFA3hH79juXll18AYNeuXbzzzr85+uj+9O9/HK+//irGGLZs2cyqVZ9XfP5rr71CSUkJjuNw003X8+9//7NB19TYVDGWtGMdB4PFiVExDvkM6PQ7EZGEGXboacleQoQTTjiR3r378Ne//qlOr5s48Souuui3fPbZp1x77Y3Mnz+HMWNGYq3llltup0WLlowdeyl33HEbY8aMpEuXA+nYcb+Y1dvqXj9x4pXMmXMbGRkZtGjRgilTptOhQwduvfVmRo++gIyMDK68chKtW7dmxoxZLFhwB4sXP0hWVjZz5swnKyurob5NAFx88Tjuvnseo0dfgDGG0aMvoXv3HnTr9nN++OE7LrxwBJ06da4I5IMGDebbb79mwoSxGONy/PEncsYZZ7Fly+YGXVdj8llrPTUUWHOMBWqeWVq8+nN2ffc1K7I3VXmuq9uKY478FTkHHdzYS5QE0OxaKad7ITVs2bKOTp0OSdr7R88xTrTXXnuFzp270LfvUWzZsoWrr57A008/j18jQhtNrHvOE3OMRRLFOm7MNgrYM5VCFWMREWkEhxzyMxYsmIsxLj6fn8mTpyU1FN9223R++OH7Ko8PGjSYceMuS8KKUp+CsaQd6zgx2yggvCHPOlWnVYiIiOyrHj2OYPHiR5O9jAozZ85O9hI8R7V9STvWdWLOMIbyinEowSsSERERL1AwlrRjHSfmqDbQVAoRERGpnoKxpB3ruDjV9Bg7PrVSiIiISGwKxpJ+3Oorxi4WN6RWChEREalKm+8k7VjHxaH6kX5BN5DA1YiING3bX3iuUa673zm/rvH5jz9eyW233cySJU/Svn0HAJ544m988cUq5sxZwEcf/ZeHH17Ejh3bMcZw2GGHM2nS9ey//wF8/PFKpky5lq5dD8Jai+OEOOec33D++b8D4KqrJpCXt43mzVsAUFxcTJcuXZk5cxYdOnRslK/XywYN6s+7766s9vlXXnmJTz75iJtvvjVxi6qGgrGkHetW30oBEHQUjEVE0l2/fv057bQzmTdvNnfeeQ+rV3/Oiy8+x6JFf+Ozzz7h9ttvYc6cBfTu3QeAv/99KdOmTeavf/0bAN279+SBB/4CQElJMRdddD7HHns8//M/hwIwZcp0+vXrD4AxhunTp/DUU49zxRWTkvDVSkNRMJa0Y12nxmAc0lQKEZEmYcKEKxg/fgzPPPMUf//700yffhutW7dmyZK/MmbMpRWhGOA3vzmfQCBAMBiscp1AIIDf76dVq9iHRpSVlVJQsIsjjugFwFdffcH9999DIFBG27btmDx5Gl26dOX7779lzpzbcF2XI488ig8+WMHTTz/PnDm3UlBQwE8/beDyyyfRsWPHmK9/6qnHWL78Zfx+Hz179uLGG2/m22+/Yf78ObiuS3Z2NtOmzeSggw7mvffeYdGiP2GtoUuXrkyePI0OHToyYsRwjjiiN998s5aFC/9aUU2PdvbZQznppCF8+eVqOnTYj2HDzubZZ58iL28b06bN5Oijj2H9+nXMnz+HwsLdNGvWnGuuuYGePXuxefMmbr/9FkpLS+nVq3fFNUtKSrjnnnl8//13GGO48MLRnHrq6fvyW9zgFIwlrVhrwxXjrOqDsWOq/tATEZH0k5WVxYwZs7j44lFcdNFYevfuC8AXX6zm6quvrfL5o0b9vuLXa9d+xdixo7DWsHHjBk455VT22y+34vl582bTrFlzdu3aSevWbfnVr07jggsuJBQKceeds5k37146derEf/7zPvPmzeG++xYye/atjB9/GQMGDOLppx/HdfduBm/bti3z599LKBRi3LjRVV5/zz1/5LHHlvD886/i9/u5885Z5OVtY+nSJxg58iJOOeVXLF/+D774YhWtWrViwYI7+NOfFtO5cxeeeOJv3HPPfGbPngeEj8W+/fa5NX7vdu7cwQknnMjkydO4+uqJvP32v1i48K8sX/4Pli59kqOPPoZZs27hoovGMmTIKaxevYrp06fw5JPLuPfe+Zx55nCGDz+XV199mRdeWAbAI48spnv3nkyffhvFxUVcdtklHHFE7xrXkWgKxpJWyk+1q27zHahiLCLSlKxa9Rlt27Zj5coPufji8WRmlkcfHwChUIjx48cAsHt3AbfddgcQ2UpRXFzE9ddP4rHHlvD7318M7G2lWLXqM6ZPv5HBg39BVlYW33//LZs2bWTq1Osq1lBcXMzu3QVs2bKZAQMGATBs2Dk888xTFZ9THhA3bFgX8/UZGRn07t2XceNGc9JJQxg58kJyc/dnwICB3HPPfP7znxUMHDiYgQNP4oMPVtCzZy86d+4CwNlnn8ejjy6p8l61OeGEgQB06tSZvn2PAuCAAzpRWLibkpISNm7cyJAhpwDQu3cf2rRpw/r16/jkk4+49dY5AJx22hnceecsAFau/JBAoIyXX34RgLKyspgn8yWTgrGklz3/9V3dyXcAIaNgLCLSFPzww/c89NCD/OlPi5k793YeeWQxl146kZ49j2DVqs849NBuZGVlsWTJE0B4U10oxuSili1bccopp7Jy5X+qPNenz5GMGDGS226bzuLFj+G64daF8mu6rkt+/k78/gysrX5jeE5Ozp7Pj/16gLlz7+aLL1bxwQcruP76ScyYMYuTT/4VvXv35b333mHp0id4//13GTjwpIhrW2sjqtPl71WbrKysil9nZGREXbPqv2etZc/7+DAm/LX6fD78/vBrjXG55ZZZdO/eAwhXpdu0acvrry+Paz2JoHFtklbsnj/4rq+mqRQKxiIi6S4QCDBz5k1cccUf6Nr1QKZPv42//30pq1ev4pJLJrJkyV/54ovVFZ//7bffsGnTT1UCIITD3ieffMThh/eI+V4XXHAhxcXFvPDCMg455Gfs3r2bzz77BICXX36RW2+9mVatWtG164G8//57ALzxxqv4fL4q16ru9fn5+Vx00W859NCfM27cZRx77PF89903zJhxE1999SXnnvsbxo27jLVr13DEEb358stVbN68CYAXX1xGv37H7Ns3NErLlq3o0qUrb731JgCrV69i584dHHpoN/r3P47XXnsFgLfeepNgMLzpvV+/Y3n++WcB2L59O2PG/I6tW7c06Lr2lSrGklas6+JiMTWMa3OsizUGn1//XSgikq7++Md7+J//6cbQoWcC4XaASZOuY9asW3j44Se47bY7WLRoIfn5OykpKeWAAw7gqquu5cgjj+bjj1dW9Bj7fOA4Dj//+eFceOGYmO+VnZ3NhAlXcP/9dzN06JnMmnUn9913F8FgkBYtWjJ9+m0ATJ9+G3Pn3s6iRQvp1u2wmJXb7OzsmK9v3749Z5/9a8aPH01OTjMOPvgQhg07hyOP7Me8ebNZsmQRmZlZ3HDDVDp06MjkyTczbdoNhEIOnTp1YurUGQ3+PZ4xYxYLFtzB4sUPkpWVzZw588nKyuK6625k1qwZvPjic/To0ZMWLVoCcMkl47n77nn8/vfnY4zhiism0bXrgRX/EZAKfLamun4K2rGjqKI8L01Xbm5r8vIKqzwe2rmTvHfe5L3sTdW+trNpyYmn/h5/dnZjLlESoLr7QJoe3QupYcuWdXTqdEjS3j8z04/jVN9Kl2wPP7yI4cN/zX777cdbb73J668vZ86cBclelqfFuuf8fh8dO8aeIFIbVYwlvbhujf3FEO4/tq6OhRYRkcQ64IBOXHvtFWRmZtK6dRumTr0laWsJBMqYOPGSmM+NGzeRQYOGJHhFqUHBWNKKdd0a+4thz8a8PdMrREREEuXMM4dz5pnDk70MAHJymlVs8JO91GQpacWaOCrGPquKsYiIiFShYCxpxToOThwVY+uqYiwi0hg8tnVJPKwx7jUFY0kvrotba8XYVBwEIiIiDSczM5vi4t0Kx9LorLUUF+8mM7NhN9Krx1jSSvm4tpo4qJVCRKQxtG+fS35+HkVFu5Ly/n6/H2NSdyqFNKzMzGzat8+t/RPrcs0GvZpIklljcHw1/1C0WJxQgPjO/RERkXhlZGSy336dk/b+Gtsn+0qtFJJe4qgYA4ScYAIWIyIiIl6iYCxpxbpOrT3GAE5Ix0KLiIhIJAVjSSvWNbVOpQAIuoEErEZERES8RMFY0oqNYyoFgOOoYiwiIiKRFIwlvRj1GIuIiEj9KBhLWrGuW+tUCoCQq2AsIiIikRSMJb24Jq6KsWN0wIeIiIhEUjCWtBKeShFHMFYrhYiIiERRMJa0Yo2Ja/NdSBVjERERiaJgLGnFui5uHOPaQkZTKURERCSSgrGkFeM6GPUYi4iISD0oGEtacd34Aq/jqmIsIiIikRSMJa24cVaCHeM28kpERETEaxSMJa048VaMrVopREREJJKCsaQNay2Oja8S7BgXa2qfXiEiIiJNh4KxpA83vuOgARyfxbpqpxAREZG9FIwlbVjXjWuGMYCLwTpqpxAREZG9FIwlfVgT1wzjcqFQoBEXIyIiIl6jYCxpwzpuXDOMy+lYaBEREalMwVjSRvg46DpUjBWMRUREpBIFY0kfJv7Nd6CKsYiIiERSMJa0YV2D64t/BFvIUY+xiIiI7KVgLGnDmrr1GKuVQkRERCpTMJb04datx9hxQ424GBEREfEaBWNJG9a4mLqMa1PFWERERCpRMJb0UdeKsaOKsYiIiOylYCxpw5r4T74DtVKIiIhIJAVjSR/G1G3zndGR0CIiIrKXgrGkDWsMTh16jDXHWERERCpTMJa0Yd06HgmtirGIiIhUomAs6aOOrRQKxiIiIlKZgrGkDevW8Uhobb4TERGRShSMJX1YU6c5xo51G3ExIiIi4jUKxpI2bJ1PvlMrhYiIiOylYCxpw7gOti7B2CoYi4iIyF4KxpI23DpupnONi7HxHwgiIiIi6U3BWNKG69axZ9gaXKM+YxEREQlTMJa04Zq6TZmwxqidQkRERCooGEvacE0d2yKsxVHFWERERPZQMJa0UdceYyyEQoHGWYyIiIh4joKxpI36VH9DoWAjrERERES8SMFY0oap6+Y7IOSoYiwiIiJhCsaSNlzqHoxdHQstIiIieyQlGL/wwgsMGzaMYcOGMW/evGQsQdKQW4+T7FQxFhERkXIJD8alpaXMmTOHRx99lBdeeIGVK1eyYsWKRC9D0lB9ZhI7jirGIiIiEpbwYOy6LsYYSktLcRwHx3HIyclJ9DIkDRlbnx5jBWMREREJy0z0G7Zq1Yo//OEPnHHGGTRv3pxjjz2Wfv36JXoZkobcehzv7KjHWERERPZIeDBes2YNf//73/nXv/5F69atueGGG1i8eDHjxo2L6/UdO7Zq5BWKV+Tmto74OCvbRxYZdbpGs+b+KtcRb9Hvn5TTvSCg+0D2TcKD8bvvvsuAAQPo2LEjAOeddx5PPPFE3MF4x44ijLGNuUTxgNzc1uTlFVZ8bK2lpDRIKKNu7RS7CgojriPeEn0fSNOle0FA94GE+f2+ehdSE95j3KNHD1asWEFJSQnWWt5880369OmT6GVIujEGl/q0UtR9koWIiIikp4RXjAcNGsSXX37JeeedR1ZWFn369GHChAmJXoakGWvqE4vVYywiIiJ7JTwYA0yYMEFhWBqWsRhfPSrGRhVjERERCdPJd5IWrFvPirFRxVhERETCFIwlPRiDoe6bMtVjLCIiIuUUjCUtWGtw6xOM97RShJy6Hw4iIiIi6UXBWNKDW7+KsTWGgpJSXvtwA6UBVY9FRESaMgVjSQvWGFxfPeZbW8u6rYU4ruGnvOKGX5iIiIh4hoKxpAdb/4rxtl0lAOQVlDb0qkRERMRDFIwlLdj6tlK4hvzCMgDyCwMNvSwRERHxEAVjSQ/1nEoRCPlx3PDGu2DIpaRMfcYiIiJNlYKxpAVrDKYePcaBoB9j976usDTYkMsSERERD1EwlvRQzx7jYMiHsXuPBikq0YEfIiIiTZWCsaSHerZSBIMZkcG4VMFYRESkqVIwlrTgOPXrDQ46kcFYPcYiIiJNl4KxpAW3nkc7O24Grtl76l1xmSrGIiIiTZWCsaQFY+t+pLPj+jDWh60UjEsDDtbW46AQERER8TwFY0kLxtS9YuyYDICIirFrLIFQ3UO2iIiIeJ+CsaQFtx49xiE3HIyNiQzCpQEFYxERkaZIwVjSgluvVorw7W+MiXi8NKANeCIiIk2RgrGkBdfUJxjvqRhHherSoIKxiIhIU6RgLGkhuh0iHo6JXTEuUyuFiIhIk6RgLGnBrc/mu4qKcVQwDioYi4iINEUKxpIW6tVKUVExjnxtmVopREREmiQFY0kLdW2lsLbS5jtVjEVERAQFY0kT0eG2Nq7xA76K11r2HuqhOcYiIiJNk4KxpAWnjj3Grql061sbcdpdMOTimroFbREREfE+BWNJC7aOQdaJCMZVK84BtVOIiIg0OQrGkhbq2kpR3l8cZqsG45AqxiIiIk2NgrGkhbpOpajcSmFt1WCsyRQiIiJNj4KxpIXoQzpqE91jbCptvgNtwBMREWmKFIwlLbjULcg60cFYPcYiIiJNnoKxpIW6zjGuUjFWj7GIiEiTp2AsacGt1xzjPWIGY1WMRUREmhoFY0kLdR3XFrH5DjA2qsdYrRQiIiJNjoKxpAXXxh9krQXX+CIeiK4YBx0FYxERkaZGwVjSQnTFtybhUBwZjC1qpRAREWnqFIwlLZg6VIwj+oshdsU4ZCKOiRYREZH0p2AsaSGuk++spVPJTlqVlVV9fVSPsrWWoCZTiIiINCkKxpIW4gnG7YLFtAqV0bKslIyo8W6xTs4LqM9YRESkSVEwFs+zxuBSS9uDtbQNlgDgWD85biji6VhzkIOaTCEiItKkKBiL9xlT5UjnaDluiCzjAODaDLKNEx5PUX6JGBVnbcATERFpWhSMxfOsrT0Yt3QCFb8OmQz81pJRKQzHrBg76jEWERFpShSMxfuMrTUYN68UjB2bAUBWpTAcvfkOdMiHiIhIU6NgLN5nDZXP64jms5ZmlXqKHRu+7TP3tFZA7FYKHfIhIiLStCgYi+fZWnqMs90QvkrPh0y4YpxZKQxbW3VucUDj2kRERJoUBWPxPOsabA3BuFnUBIryirHfWvzlLRTWYog+5EMVYxERkaZEwVg8L9YM4sqyTXQwzqj4deaeE/NszNPvFIxFRESaEgVj8TzjOjU+n+NW7iX2Yeze275iMoW1GLVSiIiINGkKxuJ5bk3B2NqIirFjI2/5jMqtFDEqxtF9xyIiIpK+FIzF84ytvuUh07r4K4Xbym0UEF0xjgzGxlpCmmUsIiLSZCgYi+cZt/pgnB1VTS6fSFHOb034BDxb3cg2BWMREZGmQsFYPK+mzXdZUc+5Ua0UPsJVY2ttzMkWOuRDRESk6VAwFs8zpvoe4+yo56J7jKG8naJqKwXokA8REZGmRMFYPK/minFUK0VUjzHsCcYxeowBgppMISIi0mQoGIvn2X1opYDKwThGK4VmGYuIiDQZCsbiedWOa7OWzKhgHD2VAsIb8GId8AE65ENERKQpUTAWz3NjBFqATGvwRW2oi91jbKttpVDFWEREpOlQMBbPq66VIjPGpjzHVK0Y+6zFb1wM6jEWERFpyhSMxfOq23wX3V8MsXuMAfzGqGIsIiLSxCkYi+cZU00rRYxgHKvHGMJ9xrGuox5jERGRpkPBWDzPreZI6MwYFeBYPcYQnkxhYgRpVYxFRESaDgVj8bxYgRaqVoyN9WGqa6WopmLsGovjqs9YRESkKVAwFs+L1RsMkGmjR7VVf7tnWFttwFbVWEREpGlQMBbPq27zXXTFuLqNd7CnYlxNwA4EFYxFRESaAgVj8Twbo8fYZ234RLtKaqoY+60F42JjnH4XdNRKISIi0hQoGIvnxaoYZ8QIy9VNpCjnc92Yh4WoYiwiItI0KBiL58XaNJcZ47GaKsYAGa4T85AP9RiLiIg0DQrG4nmxeoNjVYzd2irGxsXGuJZmGYuIiDQNCsbieSZGCI5ZMTY13+5+4+r0OxERkSYsrmB89dVXs2LFisZei0i9GFN1w1zsinEtwdhVMBYREWnK4grGp556KgsXLmTo0KEsXryYXbt2NfKyROIX6+S72Kfe1dxKEa4YVw3ZgZCmUoiIiDQFcQXjs88+m8cee4yFCxeyY8cORowYweTJk/n8888be30itbIx2iYyYp1iV1vFuLpWCk2lEBERaRLi7jE2xrBu3Tp+/PFHXNelY8eO3Hrrrdx///11ftM333yT8847jzPOOIPZs2fX+fUilcWaJBE9wxhqn0rhsxZjnCqPBx0FYxERkaYgM55Puvfee1m2bBkHHXQQo0aN4r777iMrK4uSkhJOPvlkJk2aFPcbbtiwgZkzZ/LMM8/QsWNHxowZw1tvvcWQIUPq/UVI0xZrXFt9eozBYp1gjOtbgiGX7KyaWzFERETE2+IKxjt37mTRokX06NEj4vEWLVpw99131+kN33jjDc4880w6deoEhEN3Tk5Ona4hUlnMcW0x5xjXEmwtWDcU86mAgrGIiEjaiysYu65bJRRPmjSJ+++/n0GDBtXpDdetW0dWVhaXXXYZmzdv5he/+AXXXHNN3K/v2LFVnd5P0ldubmsAsrL9ZLl7Q6vPGjIzfIAv4vNdXwY+f+RjEXyQ5Te0bFn1P9RatW5ObocWDbJuaVjl94GI7gUB3Qeyb2oMxjNnzmTr1q189NFH7Ny5s+Jxx3HYsGFDvd7QdV1WrlzJo48+SosWLbj88st57rnnOO+88+J6/Y4dRTHHc0nTkpvbmry8QgBKy4KEKm2Qy3IdjBtZMbYWXNcH1HDv+CBYVkZxcaDKU5u27sbnqtc41VS+D6Rp070goPtAwvx+X70LqTUG4xEjRvDNN9+wdu1ahg4dWvF4RkYGRx11VL3ecL/99mPAgAF06NABgF/96ld8/vnncQdjkWjRm+/89dh4F2bBid1KodPvRERE0l+NwbhPnz706dOHgQMHcsABBzTIG5588slMmTKF3bt307JlS9555x1++ctfNsi1pWmK3nwXa4Zx7RvvCBeT3apTKQDKNLJNREQk7dUYjP/whz9w3333MW7cuJjPv/TSS3V+wyOPPJJx48YxatQoQqEQAwcO5De/+U2dryNSLnrzXeyKcZwb52rYfCciIiLprcZgPH78eABuueWWBn3TESNGMGLEiAa9pjRdblQrRawZxnFVjAGMG/7HHxmkdciHiIhI+qsxLfTu3RuA4447js6dO3PcccdRUlLCf//7X3r27JmQBYrUxtrag3F8Pcbha8Ua2aaKsYiISPqLKy3MmDGDRYsW8d133zF9+nQ2btzItGnTGnttIrWy1mKjJk3sS8XYWhuznUIVYxERkfQXV1pYvXo1t956K2+88Qa//vWvmTt3Lj/99FNjr02kVsaacJitpF6He+xhrcXEOP2uTBVjERGRtBdXMLbW4vf7ee+99zjhhBMAKCsra9SFicTDWBMeUlxJrM13cfcYW4t1qwZj1zWEnKrXFRERkfQRV1o4+OCDGT9+PBs3buS4447j+uuvp3v37o29NpFaucatcmbHvvQYAzErxqA+YxERkXQX15HQc+fO5Y033uCYY44hKyuL/v37c+655zby0kRqZ0zVsLpPUymsxVZzyEcg6NKqeVad1iciIiLeEVdaaNGiBf3792f37t188cUX9O3bl++//76x1yZSKzf6QA5r8duqxz7HHYwBW03FWH3GIiIi6S2uivF9993HQw89RMeOHSse8/l8/POf/2y0hYnEo8rhHlh80b0V1OGAD2uxVrOMRUREmqK4gvELL7zA66+/3mDHQos0FBNVMY618Q7qNq7NWot1Q/iqBOPYx0WLiIhIeogrLXTu3FmhWFJSdCtFrP5iqNvmO0vsWcZqpRAREUlvcVWMBwwYwPz58/nlL39Js2bNKh7v1atXoy1MJB7Rp97F6i+GOm6+sxZibMBTK4WIiEh6iysYL1u2DIBXX3214jH1GEsqqFIxjnG4h7Fg4q4Yh0/Ss24QX9QzZQrGIiIiaS2uYPzmm2829jpE6iV6XFvswz3i3HgHYGtopVAwFhERSWtxldGKi4u5/fbbGTNmDLt27WLGjBkUFxc39tpEamWiKsQZMSdSxN9fDHs23zlVN9oFQ26V46dFREQkfcSVGGbPnk3r1q3ZsWMHOTk5FBUVMWPGjMZem0it4qsY1yEY2/BkilgVY2OtTr8TERFJY3Elhq+++oprr72WzMxMmjdvzl133cVXX33V2GsTqZVrax/XVqdgzJ4NfdZA9OEhaAOeiIhIOosrMfj9kZ/mum6Vx0SSIbpiHGtcW91aKahol7DqMxYREWlS4tp8d+yxx7JgwQLKysp45513eOyxxzj++OMbe20itTJu7ePa6loxrjhNzw0CzSOeUzAWERFJX3ElhhtuuIEWLVrQunVr/vd//5cePXpw4403NvbaRGplbTwV4zpMpWDvbGQba5axeoxFRETSVq0V4zfeeIPFixezdu1amjVrRvfu3enXrx85OTmJWJ9Ijdwqm+/2vWJcMXkiZiuFjoUWERFJVzUG4+XLl3PvvfcyadIkevTogc/nY9WqVcyZM4dAIMBpp52WqHWKxGSqnHzXEJvvLBbwOcEqz6mVQkREJH3VGIz/9re/sWTJErp06VLxWLdu3TjyyCOZNm2agrEkXcTmO2tjVozruvkOW376nTbfiYiINCU1Jobi4uKIUFzuf/7nfwgEAo22KJF4Va4Y+7D4YhzwUdeKMTZ8yAfGhaigXRZQK4WIiEi6qjExZGRUv2lJJ4BJKqhcMc6o5p6s8+a7Pf/DWqwb2U5Rps13IiIiaUvDiMXTKm++i9VfDPWpGFf6D7+oyRTG6PQ7ERGRdFVjj/HatWvp169flcettQSDVTcmiSRa5b+5iNVfDPXvMQaqmUzhkpNVtyq0iIiIpL4ag/Ebb7yRqHWI1EujVIyxlU6/C+KLerYs6NC2ZXYdrykiIiKprsZg3LVr10StQ6ReTC3B2Fgw9WmlIHYrBUBZQK0UIiIi6Ug9xuJptlIYbojDPfZctFLFWId8iIiINBUKxuJpbqVgnNEQo9r2qBgDF6tirFnGIiIiaUnBWDzNRFSMq7ZS1HVUWzlrTPkvwI2sECsYi4iIpCcFY/G0yB7jhqsYV27RiG6nUCuFiIhIelIwFk+rrWJc71YKU+la0Yd8qGIsIiKSlhSMxdNqm2Nc5xnGFdetVDGO6jMOBF2d/CgiIpKGFIzF01xb87i2hmiliD7kw1idficiIpKOFIzF0yq3PPhjTKWo7+Y7U7ki7FQ95VHtFCIiIulHwVg8zdIIc4ypefMdKBiLiIikIwVj8bSIinHMcW31DcZ2b/3ZuBAVuksDmkwhIiKSbhSMxdNMY5x8BxGn32EtVpMpRERE0p6CsXia2VPX9VmLrwF7jLEWW/l6jmYZi4iIpDsFY/Esa21FK0WsNgrYh4oxtsbJFGUBVYxFRETSjYKxeJaxpqL3N1YbBdS/xxgbOZlCp9+JiIikPwVj8axwG8WeYExDV4zBVjpuOnpkm3qMRURE0o+CsXhWbRVjY8HsQzA2lYKxdSMrxIGQi2tih3ERERHxJgVj8SxjTcUGudgTKeq58W6PmnqMQVVjERGRdKNgLJ5lrSnvpGjQGcblKs9IxrjhfyrRBjwREZH0omAsnmWsrWil8DXkDOOK60eG7egNeKXagCciIpJWFIzFswyVeoxjzDDe12Bso3uINbJNREQkrSkYi2dFbr6L1UrRgD3GgNUhHyIiImlNwVg8K9zqUP3mu33uMY4O21HHQpdq852IiEhaUTAWzzLWYmsY17bPrRRVgnF0K4UqxiIiIulEwVg8y1SeShHjgI99rxhHhm3rRAZhVYxFRETSi4KxeJZr3BoP+NjXOcZYG9lOEeNYaFvNUdQiIiLiPQrG4lmVj2xujB7j8HuYyh+A2Vs1NsYSDOn0OxERkXShYCyeVfnI5saYYxz9HlB1MoVmGYuIiKQPBWPxLLdyxbgReowBjBvVRxx9yIc24ImIiKQNBWPxrNpaKfa5xxiwNqpiXKXPWBvwRERE0oWCsXiWk4AeY1PL6XeqGIuIiKQPBWPxrPKKsc9afI1wJDRU7TGuGoxVMRYREUkXCsbiWeWhNdZx0K71Ya1v39+jyrHQkRViHQstIiKSPhSMxbPKg3GsarHTAP3FEDWuDapUjEvUSiEiIpI2FIzFs9w9obUxjoMuF10xjp5lrM13IiIi6UPBWDxrbytFjIqxaZhb21pTpR5deZax6xoCIYVjERGRdKBgLJ5lbPU9xg0xkQIIT0eODt7RI9vUTiEiIpIWFIzFs8r7f2NPpGigHmMsRJ9+pz5jERGRtKRgLJ7l1tRK0UAVY4jn9Du1UoiIiKQDBWPxrL2tFI23+Q7imWWsirGIiEg6UDAWz6rYfNeI49rC7xM1y9iNDMIKxiIiIulBwVg8q3yUmq8RN99B7RVj9RiLiIikh6QG43nz5jF16tRkLkE8rKZxbQ3aSmGjgrExERvyVDEWERFJD0kLxu+//z7PPfdcst5e0kBNB3w0ZisFRE6mKAu6mBhrEBEREW9JSjDetWsX9957L5dddlky3l7ShN3TQhGrx7ghK8Y2RqtG5XYKa61mGYuIiKSBpATjGTNmcO2119KmTZtkvL2kifKKceP3GMcIxk5Un3GZgrGIiIjXZSb6DZ955hk6d+7MgAEDWLZsWZ1f37Fjq0ZYlXhRs+YZZGVnkFXmwx8VhI0vE5/f1zBvZC2ZWRn4fHuvl5llyWqZU/FxTosccnNbN8z7SZ3o+y7ldC8I6D6QfZPwYPzKK6+Ql5fHOeecQ0FBASUlJdxxxx1MmzYtrtfv2FGEMernbOpyc1tTXFxGKOhiHRfj7q3qutZH+EyOhrlPXCAUCOHz7w3fTnEpwZxAxcc/bS6gdbaGvCRabm5r8vIKk70MSQG6FwR0H0iY3++rdyE14cH44Ycfrvj1smXL+PDDD+MOxSKVVbf5riH7iwEMFmtMRDDWsdAiIiLpRyUu8azyMWq+qGDckBMpytlajoVWj7GIiIj3JbxiXNl5553Heeedl8wliIeZaqZSNHTFGMIzkzMiHwjPMvaHHy0pC8V8nYiIiHiHKsbiWdYasDZGxbhxgnGV969UNS4Nuup9FxER8TgFY/Es1xp8WHwkIBhHn34HVWYZq89YRETE2xSMxZOMMWBsNcdBN3yPcTyzjIvVTiEiIuJpCsbiSeH+4tjB2DGJb6UAKC5VxVhERMTLFIzFk4w1WFu1jQIaafNdLcdCgzbgiYiIeJ2CsXiS2bPxzh/zOOhGaKWI8T7RFeOiUgVjERERL1MwFk9yK4JxgirGWGz0e1XpMVYrhYiIiJcpGIsnVVSMY7RSNMZUCosNzy6OeNCA2RuGi8tCmBhBXURERLxBwVg8KRyMq556B4108h175iZHP16pamyM1Ql4IiIiHqZgLJ5kbLi1IeZUikaoGAOY6GOhocoGPPUZi4iIeJeCsXiSa10gditFY/QYQ3wj24pKFIxFRES8SsFYPMlYu+c4aBP1uA+TwGCME4z4sLA0WPVzRERExBMUjMWTTDVTKRqrjQLKq9TRD0ZWiAtVMRYREfEsBWPxJGNcbIxWisZqowCwMY6Ftk50MFbFWERExKsUjMWT9rZSJK5iHPP0O+NApTWEHENpQJMpREREvEjBWDzJtW41rRQNP6qtXMxgbG2VDXi7i1U1FhER8SIFY/GkcMWYKsG4MVspYgZjADcyCBcoGIuIiHiSgrF4kmv2VIyJDKuNXTGOea6do4qxiIhIOlAwFk8y1mBj9Bg36uY7qtmAp4qxiIhIWlAwFk8q33wXPZWiMTffAdiYs4yrnn4XcqppuxAREZGUpWAsnmT2bL5LZMUYwHWrTpywUYd8WGspKAo06jpERESk4SkYiyeVB9QqUylM4/UYQzWn37khiKpc7ypSO4WIiIjXKBiLJ7mui89afAk84APAVjOyjahK8s7CskZdh4iIiDQ8BWPxJNc4VdoooPF7jN1YFWOqtlPkF6qVQkRExGsUjMWTjDFVRrVBAjbfVTfLOCoYlwYcSsp0Ap6IiIiXKBiLJ7muU6W/GMBtxDnGEA7ksURXjAF27lY7hYiIiJcoGIsnlfcYV2Zt4/cYV3v6XYxgvL1AwVhERMRLFIzFk4xxEz7DGKoPxtGHfADsUMVYRETEUxSMxZNcU7WVorGrxVDDsdCuA1GhubAkSGlAfcYiIiJeoWAsnuQaF19UEHUaub+4XKxjobE2Zp+x2ilERES8Q8FYPMm4pkorRSIqxgDGVFMFjhGM83aVNvJqREREpKEoGIsnxWqlSFTFOObpd8SeTLEtX8FYRETEKxSMxZNc48boMfYl5L2NGzsY41Q91KMs6LC7RMdDi4iIeIGCsXiSa0yV46ATVjGubjKFE4r5eJ6qxiIiIp6gYCyeZGK2UiSqx7i6inEQYsys2KY+YxEREU9QMBZPMsZUOeCjsU+9q/zeMVkDMarGOwrKMCbmkDcRERFJIQrG4kmucfETPa4tQRXj6k6/A2yMPmPHNews1Ng2ERGRVKdgLJ5kjEnKAR8AtqZgHKoajAHydikYi4iIpDoFY/Ek17pVWilSoWIcazIFwPYC9RmLiIikOgVj8Rxr7Z5WiiQd8FHdsdDEnmUMkF8YwK2uN1lERERSgoKxeI6xBqyN0UqRmM13ALbaWcZBsFVjszGW/MLY1WQRERFJDQrG4jnlwThZrRRQw8g2a2NuwAPYuVvBWEREJJUpGIvnGCzW2IipFK71YRN08h2E5yhXq5oNeDt3awOeiIhIKlMwFs8x1gUbOZUiUf3FFWuorpWC2CPbALVSiIiIpDgFY/EcYy2+qGCaqOOg966hho10odiV4UDIpag09rHRIiIiknwKxuI5xhp8URMeEl4xrq7HGLCh2JMpAHYVqWosIiKSqhSMxXNc6+Iz0RXjRAfjGirGxgE3dmVYwVhERCR1KRiL51hrk18xttVXjKH6E/AKiqqvJouIiEhyKRiL57jW4HMjp0IkumJc07HQALaaPuOCYgVjERGRVKVgLJ5jrIu/SsU40ZvvLDbGQR4VqgnGwZBLSVkNo95EREQkaRSMxXOMtRDVY5zwVgos1NBnXF0rBUBBsfqMRUREUpGCsXiOsaZKxTjRrRRQ8yxj3FCV8F5ut9opREREUpKCsXiOG2NcWzKCsa1hZBtU32dcWKJZxiIiIqlIwVg8x8QY15boVgoA1625V7i6YLy7RBVjERGRVKRgLJ4Tq5Ui0ZvvoOZDPoBqN+AVlYYwpoaNeyIiIpIUCsbiObFaKZJRMa4tGNtg7GBsjKWoTO0UIiIiqUbBWDzHWoPPJr/HuNaKcQ0b8Aq1AU9ERCTlKBiL57gxWymSsPnOGmpriLCh0piPF5WqYiwiIpJqFIzFc4w1EZvvXOvDWl/i11HLLGOovp1CkylERERSj4KxeI6J6jFORrUYwAC2plnGUO0GvEJNphAREUk5CsbiOcYa/OwNxk4SJlIAWCzUEoyrG9lWVBqq+UhpERERSTgFY/Ecx7gpUjG22FpaKXCd8D/RDxtLcVnNc5BFREQksRSMxXOMcfFXqrYmYyJFOVvLIR+gDXgiIiJeoWAsnuOaEL5KwThZFWMAt7YeY2ragKc+YxERkVSiYCyeYxwHH5UrxsnpMQYwJo52iOr6jDWZQkREJKUoGIvnGCey0prMirGxttZNdNVtwCtUK4WIiEhKUTAW70mhYGyxtY9sMy7Wqdo2oVYKERGR1KJgLJ5j3MhKazI335k4RrYBEGMDXsgxlAU1mUJERCRVKBiL50RXX5NaMfbFccgHOgFPRETECxSMxXuqVIyTuPkuzopxdSPbFIxFRERSR2Yy3vSBBx5g+fLlAAwZMoQbb7wxGcsQj7JOZJh0rS9JK9nTY2ziaaUIgLXgi1yr+oxFRERSR8IrxitWrODdd9/lueee4/nnn+eLL77gjTfeSPQyxMuiDtVIdsU4nlYKrMU6gSoPq2IsIiKSOhIejHNzc5k6dSrZ2dlkZWXRrVs3Nm3alOhliIdZN7pinMypFGBtHEdDAwSrtlOoYiwiIpI6Et5Kcdhhh1X8+scff2T58uU8+eSTiV6GeJSxBl+l1gVrkxuMYW87hc9f8zpsqBQf7SMeC4RcyoIOzbKT0tUkIiIilSTt38bffPMNEydO5MYbb+RnP/tZ3K/r2LFV4y1KUl7QDZGTEe7T9Wf4cawPfP7o1t2E8lkfmT7IqCXc+gjRrGVOlcezmmWT27FlYy0v7eXmtk72EiRF6F4Q0H0g+yYpwfijjz5i0qRJTJs2jWHDhtXptTt2FGFMzSeNSfoqcwI4ZeFeXeMaQiYTm+T7IWQNGYEQJiOr5k8MOriFJeCP7In+YUM+GfG0YkgVubmtycsrTPYyJAXoXhDQfSBhfr+v3oXUhAfjzZs3c+WVV3LvvfcyYMCARL+9eFy4lWJviEx2GwWEWyniOuQDsMFSfM0i/7DuLlafsYiISCpIeDBevHgxgUCAO++8s+KxkSNH8rvf/S7RSxEPMtbgr9RjnMxT78rFPZmCPX3GUcG4QMFYREQkJSQ8GE+fPp3p06cn+m0lTbjWjQjGqVAxNr5wMLZAra3OMU7AKywJYozF709io7SIiIjo5DvxlnDFeG8rhWOSN8O4XHg1FuLoE7ahqsHYGKuxbSIiIilAwVg8xY0a1+amwC1sCW/+i6udwrgxw/GuIgVjERGRZEt+qhCpg+hWCsck/xY2e4JxvBvwYh30sauo6ql4IiIikljJTxUidRDdSpEKPcawZwOeiX8DXjQFYxERkeRLjVQhEifXCYKt1GOcQsEYN75ZxDZGxbigOIirWcYiIiJJlRqpQiROxg1BpfM8XJv8zXdQt5FtOCFwQ5GvN5YC9RmLiIgklYKxeIpxAmD3JuPUqRgTdysFxK4a5xeqnUJERCSZUiNViMTJuCEql4xTqccYa7FxtkPYYEmVx3YqGIuIiCRVaqQKkThZNxDRSuGkSiuFb8/ItniDcaBqMN5RUHWMm4iIiCSOgrF4igmFUrSVoo4j25xglT7jsqBDcVmomheIiIhIY0uNVCESJ+OGKg7UMBZMCgVjS5yHfOyhqrGIiEhqSY1UIRIn6wYrWilSZSJFOYOFumzACxRXeWznbvUZi4iIJIuCsXiKdYOUJ+NUaaMoFx7ZFv8s4pgV492qGIuIiCRLaiULkVpYZ2+PcapMpCjn1mWWMYBxqoxtKywJEgjW4RoiIiLSYFIrWYjUwlY64CPlKsY+C9ZgK20OrI0NFFV5bLuqxiIiIkmRWslCpBbWqdxKkVo9xi572ijqcLSzLasajHcqGIuIiCSFgrF4SjgYh6VaxdjdE9jr1E4RCkR8TaA+YxERkWRJrWQhUhtn75zfVJtKAXv6jOswmQLAlu6O+Hh3URCnDpv4REREpGEoGIu3OHvHmaVaxRj2VI3rGGqjg7GxVsdDi4iIJEHqJQuRGljXqfi1Y1Lv9nUxdWulAHCC2GDk6LZ8tVOIiIgkXOolC5Ea+NwUb6Xw1e2Qj3K2eFfEx6oYi4iIJJ6CsXiG6wSxlSY+pGIrhYPBuob4B7aF2bJCqFQNzy8M1Gnsm4iIiOy71EsWItVwQ2UVh3tA6o1rK+fg1mlkGwDWYorzKz4MhlyKy5waXiAiIiINTcFYPMN1ooNxat6+Tl1PwNvDluyKaMPQPGMREZHESs1kIRKDUyUYp2jF2Gfq1WeMcSOqxruK1GcsIiKSSArG4hkmFGDvqXc+rPUld0HVcDCYelSMAWxxfkWoztcGPBERkYRSMBbPcENlFQVj16RmtbhcKOo0u7gZF1O0A4DdxUGM0QY8ERGRRFEwFs8wTqCilSKUov3F5YImVPsnVcMW7wLXwTWW3SX1DNgiIiJSZ6mdLkQqqRyMU7W/uFzIOnUe2VbBmoqqsfqMRUREEkfBWDzDDVUKxil46l1l1lqc+rZTALakAIzDrkJVjEVERBIltdOFSCXW9U7FGCDg7EO11xpMUb4qxiIiIgmkYCyeYUIBLN4JxkEnUP92CsJzjXcXlWkDnoiISIIoGItn2Mqb71K8lQLAGBfX1m9sW/gCLm5xPgXFaqcQERFJhNRPFyJ7WCdYPsbYExVjay1Bd99CrSnepXYKERGRBFEwFs+IrBinfjDGGIJu/ce2AeAEyd+2rWHWIyIiIjVSMBZPsMbBuk6lzXceuHWtxTUOrjX7dJn8rVsaaEEiIiJSEw+kCxHACWKswXpoKgXsaafYh8M+AHYXFOKWlTTQikRERKQ6CsbiCXZPMMZarPVIKwWAMYT2tc/YWPJ/+rFh1iMiIiLVUjAWbwgFKoKxa/1YfMleUXyMIWQczD4NboP8TRsrquUiIiLSOBSMxRMqKsZYQh5powCwJtxfHNrHTXj5RQFsYV5DLElERESqoWAs3hAqwzXhmcBeCsbsCcZBs2/tFLsCfty8HxtgQSIiIlIdBWPxBOMEMHuCseOV/uI9rDGEXKfi1L76KAr6CO7cjA2WNuDKREREpDIFY/EEN1gKe8aeeapiDOC6WOw+zTS2FgrKLO72dQ24MBEREalMwVg8wQ2VVGw+C5rMJK+mbsr7jAPuvp1gtyvgx2z/EbuPc5FFREQkNgVj8QQTLAXjrRnGFcp7o42Da916Xya/zI8NlmLyf2qolYmIiEglCsbiCW6wxFvHQVdmwbrhQFzqlNX7MvmB8Ig6d+t3DbIsERERiaRgLCnPWoMbKq0IxkGvVYzZG4wDbpCQcep1jYDjozjkwxbnY3Zva8jliYiICArG4gWh8EQKr/YYA+A6FcG+MFRU73C8s2xP1Xjz2gZbmoiIiIQpGEvKs6GyPafeGaz1YI8xhNspnHAYttayO1hIUah4z6El8dtRGv4jawq3Ywq2NvgyRUREmjIFY0l5NlQa3rRmwqfeWeuR46CjWCdUUTWGcFtFQbCwThvydpTt/SPr/PSFjokWERFpQArGkvqCZbjGYK31ZhtFOQsmEIgIx8YaCoNFmDgP/ygNhfuMAWxJAWb7j42xUhERkSZJwVhSng3uqRhb4+1gDGBMOBybvS0UrjV1mlaxvTSqahyq/6QLERER2UvBWFKeDZaEe3GNxyvG5YzBBMqwob0n4QWcMtw4+43zSir9sXVCOOs+beAFioiINE0KxpL69lSMrTUEbBoEYwhvxguFMGVlYC0WKHVK43ppXqm//KwTAMyuzbh5PzbKMkVERJoSBWNJeTZYgmNc8HqPcSzGhMOxMQTcYFxVY9fsnU5RztnwOaZkVyMtUkREpGlQMJaUZo0J9xjvmQMcSLdgDGBtxaa8eHuNtxRH/dE1Ls63/8EG46s6i4iISFUKxpLSbLA43EZh9pwcZ7KSvKJGsiccB9xAXFXjzcWR7RQQrqyHvn0f6wQbaZEiIiLpTcFYUltZMe6eU+8c48cxHjzcI17GYEOhuHqNg66vSjsFhEe4KRyLiIjUj4KxpDQbKAr3FxtDWbpWiyuxoRCBUBlOHId+bCiM/cfXFu0k9PW7aqsQERGpIwVjSWm2rBDHOtgmEowBTDBIcaik1iM/thRnEKwmP9uSAkJf/RtTuL3B1yciIpKuFIwlpdnSQhzjgDGUutnJXk5iGEMoWEbADdT8aRbWF1bfWmJDZYS+fg9n01dYE9+MZBERkaZMwVhSminbjWMcrHEpbSIVYwi3VJSESmrdiPdjQQZuTZ9iDe6mNYTW/BtTnN+wixQREUkzCsaSsmygGJwQoaZWMYbwlIpgkOJQcY0tFWWOr8aqccXlSgoIrXkbZ/1n2pgnIiJSDQVjSVnlB1aETAjHsU2mx7icdRxCTrDWKRXf5Fffaxx5QYO77XuCq9/A3fptxQg8ERERCVMwlpRli/OxWEJuiOJQJlhfspeUcCYYpNQpI2hC1X5O0PWxansdDj5xgjgbVhFa/TrOlm+wbvXXFhERaUoUjCVlmaIdhNwQ1nUpcpoleznJYQzWcSgKFlcbjl1rWLfb8OWO2uZYRLLBMtyNqwl+/hrOhlXYYElDrFhERMSz0vB8XUkH1g1hi3cRcINY16XQaZHsJSWNDQWxGRkUBotolpFDTkY2+Hw4xiHgBsNTO4D/boFtpS7Hd2pB88w6HITihnC3foub9z0ZHQ4mo/Ph+HJaNtJXIyIikroUjCUlmd3bwJrwEckhh0KnebKXlDwWTCCAPyeHMjdAWQ1j3NYVZLC5KMBh7ZvTpWUGHZoZmsX7p9wY3O0/4u5cT0bu/5DRuTu+zJyG+RpEREQ8QMFYUpLJ3wRAqVNGQWkGrm3iXT/GVIRjfDX3Wgdd+HJ7GesKWpCTkUPLLEtuC8P+LQwdmxkyavtWGoO79Tvc7evJOODnZBzQDV9G09r4KCIiTZOCsaQcGwpgdm3GtYYyp4ztpU24WlyZMZiyMnxZmfgyMmMHZGuxjoMxht1lpeT4snCymlNYms0PBVlk+KBdjqF1tiU7w+LzgWN8lDkQcH24FrL90CbHsH8Lhw6bvsLd9i0ZHQ/Bv98h+Ju3SfzXLSIikiBJCcYvvfQSf/rTn3AchzFjxnDhhRcmYxmSotyt34JxKQ4VU1bqsDPYPtlLSh3WYoMhLCHw+8Dvx+fzAzZ8ul3UaR8BAgTdIM1LM8j2ZeJmZpGXncWOrOxwuK7GthI/3+ZDiyzLwW1cupZ9S/Ot3+Jr0RZ/2074W+fia9k+4hqBoItjDNmZfrLq0uMsIiKSIhIejLdu3cq9997LsmXLyM7OZuTIkRx//PH8/Oc/T/RSJAWZkgLcbd8BUBDYzbr8Fpim3kZRHWPBuFhqnkdssZT4HEpxyHSCZIb8+PGRmZFJRlYOvqys8D8xKtAlIR9rdmSyZmcmbbMt7ZoV0yLzWzL93+JYH6U0o8jksDvoJ+QC1mCNS8tMl9xmDl1aunRo4ceX3Qxf89b4W3bA1yYXf7PWjfRNERERqb+EB+MVK1Zwwgkn0K5dOwCGDh3Kq6++ylVXXRXX6/3+pjfLtqkwpQXYjZ+RkdOMgmAhm8paE8xqRtu2VT/X5/dhTd3Gk0lVLoYMgvhNiEx/FhlZ2fizssCfgWtdXAyucbDWsgsoCPjxBzPI8Pnw+zLI8Pnx+ww5PkNOxU8TH5DJjlAmO3ZBThF0bGZo12wXbQp20iLvW2x2C2jVgWDz1oSatcD1hf/jJ8OXQXZGNs0ycvD74/sPosb4mWCtASeIdQJg3PDX5M/El52DL6MJncDoMfr3g4DuA9m3eyDhwXjbtm3k5uZWfLz//vvz+eefx/369u01Rip9tYIDuwLQGeiR3MWIR3Ts2CrZS5AUoXtBQPeB7JuE/x21MSbir2yttTH/CldEREREJJESHow7depEXl5excd5eXnsv//+iV6GiIiIiEiEhAfjE088kffff5+dO3dSWlrK66+/zuDBgxO9DBERERGRCAnvMT7ggAO49tprGT16NKFQiBEjRtC3b99EL0NEREREJILPWqut/SIiIiLS5GlArIiIiIgICsYiIiIiIoCCsYiIiIgIoGAsIiIiIgKkeDDetGkTF154IaeffjqXX345xcXFVT7np59+4uijj+acc87hnHPO4dJLL03CSqUxvPTSS5x55pmcdtppPP7441We/+qrrzjvvPMYOnQoN998M47jJGGVkgi13QsPPPAAJ598csXPgVifI+mhqKiIs846i40bN1Z5Tj8Tmpaa7gX9TGgaHnjgAYYNG8awYcOYP39+lefr9TPBprAJEybYf/zjH9Zaax944AE7f/78Kp/z6quv2ltuuSXRS5NGtmXLFnvyySfb/Px8W1xcbIcPH26/+eabiM8ZNmyY/eSTT6y11t5000328ccfT8JKpbHFcy9MnDjRfvzxx0laoSTKp59+as866yzbq1cvu2HDhirP62dC01HbvaCfCenvvffesxdccIENBAI2GAza0aNH29dffz3ic+rzMyFlK8ahUIj//ve/DB06FIDzzjuPV199tcrnrVq1iq+//ppzzjmH0aNHs3bt2kQvVRrBihUrOOGEE2jXrh0tWrRg6NChEb//P/30E2VlZRx11FFA9feHeF9t9wLA6tWrefDBBxk+fDi33347gUAgSauVxrR06VJmzpwZ87RU/UxoWmq6F0A/E5qC3Nxcpk6dSnZ2NllZWXTr1o1NmzZVPF/fnwkpG4zz8/Np1aoVmZnhM0hyc3PZunVrlc/Lycnh7LPP5rnnnuPSSy/lyiuvJBgMJnq50sC2bdtGbm5uxcf7779/xO9/9PPV3R/ifbXdC8XFxfTs2ZPJkyfz3HPPsXv3bhYuXJiMpUojmzNnDv3794/5nH4mNC013Qv6mdA0HHbYYRWh98cff2T58uUMGTKk4vn6/kxIiWC8fPlyBg8eHPHP9ddfj8/ni/i86I8Brr76akaNGoXf72fIkCG0aNGC77//PlFLl0ZijIn4/bbWRnxc2/OSPmr7vW7ZsiWLFi2iW7duZGZmcskll/DWW28lY6mSRPqZIOX0M6Fp+eabb7jkkku48cYb+dnPflbxeH1/JqREMD7jjDN4++23I/556KGHKCwsxHVdAPLy8mL+lcmjjz5Kfn5+xcfW2ooqs3hXp06dyMvLq/g4+vc/+vnt27dX+1dq4m213QubNm3i2WefrfhYPwOaJv1MkHL6mdB0fPTRR4wdO5brr7+eX//61xHP1fdnQkoE41iysrLo378/r7zyCgDPP/88gwcPrvJ5//3vfyv+AHz44YcYYzj00EMTulZpeCeeeCLvv/8+O3fupLS0lNdffz3i979r167k5OTw0UcfAfDCCy/EvD/E+2q7F5o1a8aCBQvYsGED1loef/xxTj311CSuWJJBPxOknH4mNA2bN2/myiuv5K677mLYsGFVnq/vz4SUDcYAM2fOZOnSpZx55pmsXLmSa665BoAnn3yS++67D4Cbb76ZFStWcNZZZzFv3jzuvvtu/P6U/rIkDgcccADXXnsto0eP5txzz+Wss86ib9++jB8/nlWrVgFw1113MXfuXE4//XRKSkoYPXp0klctjaG2e6FDhw7cfvvtXH755Zx++ulYa7n44ouTvWxJEP1MkHL6mdC0LF68mEAgwJ133lkxlu/JJ5/c558JPmutbezFi4iIiIikOpVWRURERERQMBYRERERARSMRUREREQABWMREREREUDBWEREREQEUDAWEYmwceNGunfvzjPPPBPx+OLFi5k6dWrC17Ns2TImTpwI7B1P2RCS9fWIiKQyHQUjIhLF7/czb948jjnmmJQ6MGjOnDnJXoKISFpTMBYRidKsWTMuvvhibrjhBp566imys7Mjni8sLOS2225jzZo1+Hw+TjrpJK677joyMzPp3bs3v/zlL1mzZg133XUXo0aN4uKLL2bFihWUlJRw1VVX8eqrr/L111+z//778+c//5kWLVrw7LPP8vTTTxMKhSgoKGD8+PGMGjUq4n1///vfc+GFF/KrX/2KWbNm8fHHH5OVlcWBBx7I3LlzadmyJR9//DF33XUXpaWl+P1+rrrqKk4++WRCoRCzZ89mxYoVdOzYkY4dO9K6deuYX/8zzzzDk08+iTGGdu3accstt9CtWzemTp3Krl272LBhA7/4xS/YsWNHxMeXXXZZ3N+XPn36NNrvn4hIfSkYi4jEcPnll/P+++9z7733MmXKlIjnZs+eTbt27XjppZcIhUJcfvnlPPTQQ0yYMIFQKMTJJ59ccTpnMBhkv/3249lnn+Uvf/kL06dPZ/ny5eTm5jJixAj++c9/csopp/DMM8/wl7/8hfbt2/Ppp59y8cUXVwnG5T799FM+/PBDXnnlFXw+HwsWLGDt2rV069aNm266icWLF3PggQeydetWzj//fLp3784bb7zBjz/+yMsvv4zjOFx00UUxg/GHH37I888/z+OPP07z5s159913ueqqq1i+fDkAZWVlvPzyywBMnTo14uMpU6bE/X0REUlFCsYiIjH4/X4WLFjAueeey6BBgyKee/vtt3nyySfx+XxkZ2czcuRIHnnkESZMmABA//79Iz5/6NChABx88MEcfvjhHHDAAQAceOCBFBQU0LJlS/785z/z1ltv8eOPP7JmzRpKSkqqXdvhhx9ORkYGv/3tbxk0aBBDhw6lb9++vPXWW+Tl5XHllVdWfK7P52Pt2rW8//77nHXWWWRnZ5Odnc3w4cNZu3ZtlWv/+9//Zt26dYwcObLisd27d7Nr1y4AjjnmmIjPr/xxXb8vIiKpRsFYRKQanTt35rbbbmPKlCmce+65FY8bY/D5fBEfO45T8XGLFi0irpOVlRXz1+W2bNnCBRdcwPnnn88xxxzD6aefzr/+9a9q19WmTRteeOEFPv74Yz744AOuueYaLr30Ujp37ky3bt0iNg5u3bqVDh068PTTT0dcIyMjI+a1jTGcc845TJ48ueLjbdu20bZt25hfW+WP6/p9ERFJNZpKISJSg9NPP53BgwfzyCOPVDw2aNAgHnvsMay1BINBli5dyoknnljv91i9ejUdOnTgiiuuYNCgQRWh2HXdmJ//r3/9i7Fjx3L00Udz9dVXc+6557J69WqOOuoo1q1bx3//+18AvvrqK4YOHcrWrVs56aSTeP755wkEAgQCAV555ZWY1x40aBAvv/wy27ZtA+DJJ59kzJgxcX0dDf19ERFJNFWMRURqMX36dD766KOIj2fPns3w4cMJhUKcdNJJXHbZZfW+/sCBA3n22Wc5/fTT8fl8HHfccXTo0IF169bF/PzBgwfz9ttvc9ZZZ9GiRQvatm3LrFmz6NChA/fffz/z588nEAhgrWX+/PkceOCBjBw5kvXr13PWWWfRrl07DjnkkJjXHjRoEOPHj+eSSy7B5/PRqlUrHnjggYhKcHUa+vsiIpJoPmutTfYiRERERESSTa0UIiIiIiIoGIuIiIiIAArGIiIiIiKAgrGIiIiICKBgLCIiIiICKBiLiIiIiAAKxiIiIiIigIKxiIiIiAgA/w9KyJTA9Gyj9gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 842.4x595.44 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot kde plot of normalised errors if computed\n",
    "plt.xlim(-0.5, 2)\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})\n",
    "\n",
    "#Kde plot\n",
    "sns_plot = sns.kdeplot(\n",
    "   data=error_df[error_df['model']!='NNRegressor_model'], x=\"Normalised error\", hue=\"model\",\n",
    "   fill=True, common_norm=False,\n",
    "   alpha=.5, linewidth=0,gridsize = 1000\n",
    ")\n",
    "\n",
    "#Save kde plot\n",
    "fig = sns_plot.get_figure()\n",
    "fig.savefig(\"100_point_normalised_error.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegressor: 0.10452657615442021\n",
      "0.06072098144138995\n",
      "Tree Regressor: 0.09686189840165131\n",
      "0.07297235943878225\n",
      "Random Forest: 0.06854643358100515\n",
      "0.05182237193666468\n",
      "XGBoost: 0.06744905447323993\n",
      "0.049615201688543685\n",
      "Neural Network: 0.13062314072662023\n",
      "0.07837223671435789\n"
     ]
    }
   ],
   "source": [
    "#Display mean and standard deviation of normalised error for each module\n",
    "print('LinearRegressor:',error_df['LinearRegressor_model'].mean())\n",
    "print(error_df['LinearRegressor_model'].std())\n",
    "\n",
    "print('Tree Regressor:',error_df['TreeRegressor_model'].mean())\n",
    "print(error_df['TreeRegressor_model'].std())\n",
    "\n",
    "print('Random Forest:',error_df['RFRegressor_model'].mean())\n",
    "print(error_df['RFRegressor_model'].std())\n",
    "\n",
    "print('XGBoost:',error_df['XGBRegressor_model'].mean())\n",
    "print(error_df['XGBRegressor_model'].std())\n",
    "\n",
    "print('Neural Network:',error_df['NNRegressor_model'].mean())\n",
    "print(error_df['NNRegressor_model'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot 2D visual representation of predicted and actual bifurcation points, along with corresponding connection points\n",
    "plotdf = testdf.sample(n=3)\n",
    "\n",
    "#Iterate over the n samples in the test data set\n",
    "for index, row in plotdf.iterrows():\n",
    "    \n",
    "    #Specify x and y values of connection points\n",
    "    x = [row['Px'],row['D1x'],row['D2x']]\n",
    "    y = [row['Py'],row['D1y'],row['D2y']]\n",
    "\n",
    "    #Specify x and y values of bifurcation prediction \n",
    "    pred_X = row['Bifx_pred_XGBRegressor_model']\n",
    "    pred_Y = row['Bify_pred_XGBRegressor_model']\n",
    "\n",
    "    #Specify x and y values of actual bifurcation point\n",
    "    X = row['Bifx']\n",
    "    Y = row['Bify']\n",
    "\n",
    "    #Create a figure\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(111)\n",
    "\n",
    "    #Plot all points\n",
    "    ax1.scatter(x ,y, s=30, c='b', marker=\"s\", label='Connection points')\n",
    "    ax1.scatter(X,Y, s=30, c='k', marker=\"o\", label='actual')\n",
    "    ax1.scatter(pred_X,pred_Y, s=30, c='r', marker=\"o\", label='pred')\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "    #Save figure\n",
    "    fig.savefig(\"1000_points_norm_sample_connection_sample\" + str(index) + \".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection\n",
    "We will now use KBest feature selection and recursive feature elimination to find the most relevant features that give the highest CV score.\n",
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN count: 0\n",
      "(99000, 22)\n",
      "NaN count: 0\n",
      "NaN count: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(66250, 1)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read dataset path as pandas dataframe\n",
    "load_path = (os.getcwd() + \"/ProcessedData/WP2T1/1000_seg_enriched_dataset.csv\")\n",
    "df = pd.read_csv(load_path,dtype={'beta': object})\n",
    "df.head()\n",
    "\n",
    "print('NaN count:',df.isna().values.sum())\n",
    "print(df.shape)\n",
    "df = df.dropna()\n",
    "print('NaN count:',df.isna().values.sum())\n",
    "df.shape\n",
    "\n",
    "#Remove rows containing 'i' \n",
    "df = df[~df['beta'].str.contains(\"i\")]\n",
    "\n",
    "#Convert column 'beta' into a column of type float\n",
    "df.beta = df.beta.astype(float)\n",
    "\n",
    "#Define x and y range of allocated area\n",
    "rangex = df[['Px','D1x','D2x']].max(axis=1) - df[['Px','D1x','D2x']].min(axis=1)\n",
    "rangey = df[['Py','D1y','D2y']].max(axis=1) - df[['Py','D1y','D2y']].min(axis=1)\n",
    "\n",
    "#Normalise connection points\n",
    "df[['Bifx','Px','D1x','GPx','P2x']] = df[['Bifx','Px','D1x','GPx','P2x']].divide(rangex,axis=0)\n",
    "df[['Bify','Py','D1y','GPy','P2y']] = df[['Bify','Py','D1y','GPy','P2y']].divide(rangey,axis=0)\n",
    "\n",
    "#Remove NaN values\n",
    "print('NaN count:',df.isna().values.sum())\n",
    "df = df.dropna()\n",
    "\n",
    "#Identify bifurcation point as target y (THIS CAN BE SET TO BIFX OR BIFY)\n",
    "labels = ['Bifx']\n",
    "y = df[labels] \n",
    "\n",
    "#identify features x\n",
    "features = ['D1x', 'D1y', 'Px', 'Py','Radius','circleradius','totseg','P2x','P2y','GPx','GPy','OrigSeg','l1','l2','l3','beta','alpha']\n",
    "X = df[features]\n",
    "\n",
    "#train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33,random_state = 0)\n",
    "\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Univariate feature selection\n",
    "This finds the strength of linear correlation of individual features to a specified label (e.g. Bifx/Bify) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Px</td>\n",
       "      <td>610343.493110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D1x</td>\n",
       "      <td>210995.841836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GPx</td>\n",
       "      <td>3803.024221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>P2x</td>\n",
       "      <td>493.133801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Py</td>\n",
       "      <td>68.057140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>beta</td>\n",
       "      <td>62.555622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>alpha</td>\n",
       "      <td>44.890329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>totseg</td>\n",
       "      <td>29.271516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>l3</td>\n",
       "      <td>21.848934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Radius</td>\n",
       "      <td>16.757452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>circleradius</td>\n",
       "      <td>14.749466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D1y</td>\n",
       "      <td>10.782885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P2y</td>\n",
       "      <td>5.658607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>l1</td>\n",
       "      <td>3.111744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GPy</td>\n",
       "      <td>0.641919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>OrigSeg</td>\n",
       "      <td>0.107869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>l2</td>\n",
       "      <td>0.000465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Feature         Scores\n",
       "2             Px  610343.493110\n",
       "0            D1x  210995.841836\n",
       "9            GPx    3803.024221\n",
       "7            P2x     493.133801\n",
       "3             Py      68.057140\n",
       "15          beta      62.555622\n",
       "16         alpha      44.890329\n",
       "6         totseg      29.271516\n",
       "14            l3      21.848934\n",
       "4         Radius      16.757452\n",
       "5   circleradius      14.749466\n",
       "1            D1y      10.782885\n",
       "8            P2y       5.658607\n",
       "12            l1       3.111744\n",
       "10           GPy       0.641919\n",
       "11       OrigSeg       0.107869\n",
       "13            l2       0.000465"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Carry out univariate feature selection using function SelectKBest\n",
    "selected_features = SelectKBest(f_regression, k = 'all').fit(X, np.ravel(y))\n",
    "\n",
    "#Create dataframe based on the feature and their score\n",
    "selected_features_df = pd.DataFrame({'Feature':list(X.columns),\n",
    "                                    'Scores':selected_features.scores_})\n",
    "\n",
    "#Sort in order of importance\n",
    "selected_features_df.sort_values(by='Scores',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are high linear correlations between the parent and sibling coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive feature elimination\n",
    "We first use RFE to find the importance of features (by rank) for the XGBoost model, and then prune out unecessary features based on their effects in the cross validation score using RFECV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import relevant modules\n",
    "from sklearn.feature_selection import RFE\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#Define XGBoost model\n",
    "model = XGBRegressor(n_estimators = 128)\n",
    "\n",
    "#Use RFE to rank features based on importance to the model\n",
    "rfe = RFE(estimator = model, step = 1,n_features_to_select=1)\n",
    "rfe = rfe.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Py</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D1y</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Radius</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>l3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>OrigSeg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Px</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D1x</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GPy</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>P2y</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>l1</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>alpha</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>P2x</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>beta</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>GPx</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>circleradius</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>l2</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>totseg</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Feature  Ranking\n",
       "3             Py        1\n",
       "1            D1y        2\n",
       "4         Radius        3\n",
       "14            l3        4\n",
       "11       OrigSeg        5\n",
       "2             Px        6\n",
       "0            D1x        7\n",
       "10           GPy        8\n",
       "8            P2y        9\n",
       "12            l1       10\n",
       "16         alpha       11\n",
       "7            P2x       12\n",
       "15          beta       13\n",
       "9            GPx       14\n",
       "5   circleradius       15\n",
       "13            l2       16\n",
       "6         totseg       17"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Print dataframe to show features and their ranking\n",
    "selected_rfe_features = pd.DataFrame({'Feature':X.columns,\n",
    "                                      'Ranking':rfe.ranking_})\n",
    "selected_rfe_features.sort_values(by='Ranking')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This also shows strong importance in parent and sibling connection points, but radius, l3 and origseg also look relatively importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recursive feature elimination with cross validation\n",
    "Here, we recursively eliminate features and observe the change in a 10-fold cross validation score. This can be used to identify how many features to keep. Sometimes the cross validation score decreases if there are features irrelevant to the output, other times its just better to remove features if they have little impact; this is to optimise computational performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 17 features.\n",
      "Fitting estimator with 16 features.\n",
      "Fitting estimator with 15 features.\n",
      "Fitting estimator with 14 features.\n",
      "Fitting estimator with 13 features.\n",
      "Fitting estimator with 12 features.\n",
      "Fitting estimator with 11 features.\n",
      "Fitting estimator with 10 features.\n",
      "Fitting estimator with 9 features.\n",
      "Fitting estimator with 8 features.\n",
      "Fitting estimator with 7 features.\n",
      "Fitting estimator with 6 features.\n",
      "Fitting estimator with 5 features.\n",
      "Fitting estimator with 4 features.\n",
      "Fitting estimator with 3 features.\n",
      "Fitting estimator with 2 features.\n",
      "Fitting estimator with 17 features.\n",
      "Optimal number of features : 16\n",
      "Best features : Index(['D1x', 'D1y', 'Px', 'Py', 'Radius', 'circleradius', 'P2x', 'P2y', 'GPx',\n",
      "       'GPy', 'OrigSeg', 'l1', 'l2', 'l3', 'beta', 'alpha'],\n",
      "      dtype='object')\n",
      "[-0.14940099 -0.06594316 -0.06361193 -0.05862569 -0.05566553 -0.05272392\n",
      " -0.05047936 -0.05002956 -0.04996648 -0.04978307 -0.04962106 -0.049487\n",
      " -0.0493753  -0.04928511 -0.04914552 -0.04902845 -0.04902845]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Recursive feature elimination with cross validation, step = 1 means drop 1 feature every iteration)\n",
    "rfecv = RFECV(estimator=model, step=1, cv=10, scoring='neg_mean_absolute_error',verbose = 1)\n",
    "rfecv = rfecv.fit(X, y)\n",
    "\n",
    "#Display the optimal number of features for best accuracy\n",
    "print('Optimal number of features :', rfecv.n_features_)\n",
    "print('Best features :', X_train.columns[rfecv.support_])\n",
    "print(rfecv.grid_scores_)\n",
    "\n",
    "#Plot graph to show the effect of features on cross validation score\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score of number of selected features\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although 16 features were found to be the optimum, we can see from the graph that the change in cross validation score is negligible after 8 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Tuning\n",
    "We will tune the parameters of the XGBoost model in the following order:\n",
    "\n",
    "* Number of trees in forest\n",
    "* Tree depth and minimum child weight\n",
    "* Learning rate\n",
    "\n",
    "#### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN count: 0\n",
      "(99000, 22)\n",
      "NaN count: 0\n",
      "NaN count: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(66250, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBoost on dataset, Tune n_estimators\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot\n",
    "\n",
    "#Read dataset path as pandas dataframe\n",
    "load_path = (os.getcwd() + \"/ProcessedData/WP2T1/1000_seg_enriched_dataset.csv\")\n",
    "df = pd.read_csv(load_path,dtype={'beta': object})\n",
    "df.head()\n",
    "\n",
    "print('NaN count:',df.isna().values.sum())\n",
    "print(df.shape)\n",
    "df = df.dropna()\n",
    "print('NaN count:',df.isna().values.sum())\n",
    "df.shape\n",
    "\n",
    "#Remove rows containing 'i' \n",
    "df = df[~df['beta'].str.contains(\"i\")]\n",
    "#Convert column 'beta' into a column of type float\n",
    "df.beta = df.beta.astype(float)\n",
    "\n",
    "rangex = df[['Px','D1x','D2x']].max(axis=1) - df[['Px','D1x','D2x']].min(axis=1)\n",
    "rangey = df[['Py','D1y','D2y']].max(axis=1) - df[['Py','D1y','D2y']].min(axis=1)\n",
    "\n",
    "df[['Bifx','Px','D1x','GPx','P2x']] = df[['Bifx','Px','D1x','GPx','P2x']].divide(rangex,axis=0)\n",
    "df[['Bify','Py','D1y','GPy','P2y']] = df[['Bify','Py','D1y','GPy','P2y']].divide(rangey,axis=0)\n",
    "\n",
    "print('NaN count:',df.isna().values.sum())\n",
    "df = df.dropna()\n",
    "\n",
    "#Identify bifurcation points as target y\n",
    "labels = ['Bifx']\n",
    "y = df[labels] \n",
    "\n",
    "#identify features x\n",
    "features = ['D1x', 'D1y', 'Px', 'Py','Radius','circleradius','totseg','P2x','P2y','GPx','GPy','OrigSeg','l1','l2','l3','beta','alpha']\n",
    "X = df[features]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33,random_state = 0)\n",
    "\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 14 candidates, totalling 140 fits\n",
      "Best: -0.052327 using {'n_estimators': 175}\n",
      "-0.052807 (0.000492) with: {'n_estimators': 50}\n",
      "-0.052549 (0.000547) with: {'n_estimators': 75}\n",
      "-0.052417 (0.000543) with: {'n_estimators': 100}\n",
      "-0.052346 (0.000600) with: {'n_estimators': 125}\n",
      "-0.052335 (0.000586) with: {'n_estimators': 150}\n",
      "-0.052327 (0.000619) with: {'n_estimators': 175}\n",
      "-0.052365 (0.000611) with: {'n_estimators': 200}\n",
      "-0.052425 (0.000616) with: {'n_estimators': 225}\n",
      "-0.052467 (0.000608) with: {'n_estimators': 250}\n",
      "-0.052503 (0.000608) with: {'n_estimators': 275}\n",
      "-0.052557 (0.000585) with: {'n_estimators': 300}\n",
      "-0.052618 (0.000584) with: {'n_estimators': 325}\n",
      "-0.052673 (0.000555) with: {'n_estimators': 350}\n",
      "-0.052716 (0.000574) with: {'n_estimators': 375}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# grid search for 50 to 400 trees in increments of 25\n",
    "model = XGBRegressor()\n",
    "n_estimators = range(50, 400, 25)\n",
    "param_grid = dict(n_estimators=n_estimators)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_mean_absolute_error\", n_jobs=-1, cv=10,verbose=1)\n",
    "grid_result = grid_search.fit(X, y)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "    \n",
    "# plot cross validation score and error bar for each iteration\n",
    "pyplot.errorbar(n_estimators, means, yerr=stds)\n",
    "pyplot.title(\"XGBoost n_estimators vs Log Loss\")\n",
    "pyplot.xlabel('n_estimators')\n",
    "pyplot.ylabel('Log Loss')\n",
    "pyplot.savefig('n_estimators.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max tree depth and minimum child weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 35 candidates, totalling 350 fits\n",
      "Best: -0.052313 using {'max_depth': 6, 'min_child_weight': 9}\n"
     ]
    }
   ],
   "source": [
    "# grid search using the optimum number of trees\n",
    "model = XGBRegressor(n_estimators = 175)\n",
    "\n",
    "#define grid search area\n",
    "parameters = {\n",
    "    'max_depth': [2 4 6 8 10 12 14]\n",
    "    'min_child_weight':[1,3,5,7,9]\n",
    "}\n",
    "\n",
    "#Carry out grid search with 10-fold cross validation\n",
    "grid_search = GridSearchCV(model, param_grid = parameters, scoring=\"neg_mean_absolute_error\", n_jobs=-1, cv=10,verbose=1)\n",
    "grid_result = grid_search.fit(X, y)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n",
      "Best: -0.051875 using {'learning_rate': 0.1}\n"
     ]
    }
   ],
   "source": [
    "# grid search\n",
    "model = XGBRegressor(n_estimators = 175,max_depth = 6, min_child_weight = 9)\n",
    "parameters = {\n",
    "    'learning_rate': [0.5,0.3,0.1,0.05,0.03,0.01]\n",
    "}\n",
    "grid_search = GridSearchCV(model, param_grid = parameters, scoring=\"neg_mean_absolute_error\", n_jobs=-1, cv=10,verbose=1)\n",
    "grid_result = grid_search.fit(X, y)\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NaN count: 0\n",
      "(299000, 22)\n",
      "NaN count: 0\n",
      "NaN count: 3\n"
     ]
    }
   ],
   "source": [
    "#Read dataset path as pandas dataframe\n",
    "load_path = (os.getcwd() + \"/ProcessedData/WP2T1/3000_seg_enriched_dataset.csv\")\n",
    "df = pd.read_csv(load_path,dtype={'beta': object})\n",
    "df.head()\n",
    "\n",
    "print('NaN count:',df.isna().values.sum())\n",
    "print(df.shape)\n",
    "df = df.dropna()\n",
    "print('NaN count:',df.isna().values.sum())\n",
    "df.shape\n",
    "\n",
    "#Remove rows containing 'i' \n",
    "df = df[~df['beta'].str.contains(\"i\")]\n",
    "#Convert column 'beta' into a column of type float\n",
    "df.beta = df.beta.astype(float)\n",
    "\n",
    "rangex = df[['Px','D1x','D2x']].max(axis=1) - df[['Px','D1x','D2x']].min(axis=1)\n",
    "rangey = df[['Py','D1y','D2y']].max(axis=1) - df[['Py','D1y','D2y']].min(axis=1)\n",
    "\n",
    "df[['Bifx','Px','D1x','GPx','P2x']] = df[['Bifx','Px','D1x','GPx','P2x']].divide(rangex,axis=0)\n",
    "df[['Bify','Py','D1y','GPy','P2y']] = df[['Bify','Py','D1y','GPy','P2y']].divide(rangey,axis=0)\n",
    "\n",
    "print('NaN count:',df.isna().values.sum())\n",
    "df = df.dropna()\n",
    "\n",
    "#Identify bifurcation points as target y\n",
    "labels = ['Bifx','Bify']\n",
    "y = df[labels] \n",
    "\n",
    "#identify features x\n",
    "features = ['D1x', 'D1y', 'Px', 'Py','Radius','GPx','GPy','OrigSeg','l3']\n",
    "X = df[features]\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "#Define regression models\n",
    "\n",
    "XGBRegressor_model = MultiOutputRegressor(XGBRegressor(n_estimators = 175,learning_rate = 0.1,max_depth= 6, min_child_weight = 9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10 fold cross validation\n",
    "score = -(cross_val_score(XGBRegressor_model,X,y,cv=10,scoring='neg_mean_absolute_error'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f9b93e1da8cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'score' is not defined"
     ]
    }
   ],
   "source": [
    "#Print cross validation score\n",
    "print(score.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
